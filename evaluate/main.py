"""
è¯„æµ‹ä¸»è„šæœ¬
æ”¯æŒå¤šç§ç”¨æˆ·ç”»åƒã€å¤šç§æ¨¡å‹çš„è¯„æµ‹
"""
import os
import sys
import json
import argparse
import logging
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, parent_dir)
sys.path.insert(0, current_dir)

try:
    from evaluate.data_loader import load_and_validate
    from evaluate.prompts import get_prompt, get_all_profiles
    from evaluate.model_api import call_model_api, extract_answer_from_response
    from evaluate.judge import judge_answer
    from evaluate.config import MODEL_DEFINITIONS, get_eval_models, USER_PROFILES, EVAL_CONFIG
except ImportError:
    # å¦‚æœä½œä¸ºæ¨¡å—å¯¼å…¥å¤±è´¥ï¼Œå°è¯•ç›´æ¥å¯¼å…¥
    from data_loader import load_and_validate
    from prompts import get_prompt, get_all_profiles
    from model_api import call_model_api, extract_answer_from_response
    from judge import judge_answer
    from config import MODEL_DEFINITIONS, get_eval_models, USER_PROFILES, EVAL_CONFIG


# å…¨å±€å˜é‡ï¼šç”¨äºè¯¦ç»†æ—¥å¿—è®°å½•
DETAILED_LOG_FILE = None
LOG_MODE = "detailed"
log_lock = threading.Lock()  # æ—¥å¿—æ–‡ä»¶å†™å…¥é”


def log_model_response_detailed(
    question_id: str,
    model_name: str,
    profile: str,
    prompt: str,
    raw_response: Dict[str, Any],
    round_key: Optional[str] = None
):
    """
    è®°å½•æ¨¡å‹å“åº”çš„è¯¦ç»†æ—¥å¿—ï¼ˆå‚è€ƒ module2/logger.pyï¼‰
    
    Args:
        question_id: é—®é¢˜ID
        model_name: æ¨¡å‹åç§°
        profile: ç”¨æˆ·ç”»åƒ
        prompt: å®Œæ•´æç¤ºè¯
        raw_response: åŸå§‹APIå“åº”
        round_key: è½®æ¬¡é”®ï¼ˆå¤šè½®é—®é¢˜æ—¶ä½¿ç”¨ï¼‰
    """
    global DETAILED_LOG_FILE
    if DETAILED_LOG_FILE is None:
        return
    
    with log_lock:
        try:
            DETAILED_LOG_FILE.write("-" * 80 + "\n")
            if round_key:
                DETAILED_LOG_FILE.write(f"ğŸ“ æ¨¡å‹å“åº” - {model_name} ({profile}) - {round_key} - question_id: {question_id}\n")
            else:
                DETAILED_LOG_FILE.write(f"ğŸ“ æ¨¡å‹å“åº” - {model_name} ({profile}) - question_id: {question_id}\n")
            DETAILED_LOG_FILE.write(f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            DETAILED_LOG_FILE.write("-" * 80 + "\n")
            
            # è®°å½•å®Œæ•´çš„æœ€ç»ˆæç¤ºè¯
            if prompt:
                DETAILED_LOG_FILE.write("ğŸ“‹ æœ€ç»ˆæäº¤ç»™æ¨¡å‹çš„å®Œæ•´æç¤ºè¯:\n")
                DETAILED_LOG_FILE.write("-" * 80 + "\n")
                DETAILED_LOG_FILE.write(prompt)
                DETAILED_LOG_FILE.write("\n")
                DETAILED_LOG_FILE.write("-" * 80 + "\n")
            
            # è®°å½•å®Œæ•´å“åº”å¯¹è±¡
            if raw_response:
                DETAILED_LOG_FILE.write("å®Œæ•´å“åº”å¯¹è±¡:\n")
                DETAILED_LOG_FILE.write(json.dumps(raw_response, indent=2, ensure_ascii=False, default=str))
                DETAILED_LOG_FILE.write("\n")
            else:
                DETAILED_LOG_FILE.write("âš ï¸ æ— åŸå§‹å“åº”å¯¹è±¡\n")
            
            DETAILED_LOG_FILE.write("=" * 80 + "\n\n")
            DETAILED_LOG_FILE.flush()
        except Exception as e:
            logging.warning(f"å†™å…¥æ¨¡å‹å“åº”è¯¦ç»†æ—¥å¿—å¤±è´¥: {e}")


def log_judge_response_detailed(
    question_id: str,
    model_name: str,
    profile: str,
    model_answer: str,
    gt_answer: str,
    is_match: bool,
    reasoning: str,
    judge_time: float,
    raw_response: Optional[Dict[str, Any]],
    prompt: str = "",
    round_key: Optional[str] = None
):
    """
    è®°å½•è£åˆ¤æ¨¡å‹å“åº”çš„è¯¦ç»†æ—¥å¿—ï¼ˆå‚è€ƒ module2/logger.pyï¼‰
    
    Args:
        question_id: é—®é¢˜ID
        model_name: è¢«è¯„åˆ¤çš„æ¨¡å‹åç§°
        profile: ç”¨æˆ·ç”»åƒ
        model_answer: æ¨¡å‹ç­”æ¡ˆ
        gt_answer: æ ‡å‡†ç­”æ¡ˆ
        is_match: æ˜¯å¦åŒ¹é…
        reasoning: è¯„åˆ¤ç†ç”±
        judge_time: è¯„åˆ¤è€—æ—¶
        raw_response: åŸå§‹APIå“åº”
        prompt: æœ€ç»ˆæäº¤ç»™è£åˆ¤æ¨¡å‹çš„å®Œæ•´æç¤ºè¯
        round_key: è½®æ¬¡é”®ï¼ˆå¤šè½®é—®é¢˜æ—¶ä½¿ç”¨ï¼‰
    """
    global DETAILED_LOG_FILE
    if DETAILED_LOG_FILE is None:
        return
    
    with log_lock:
        try:
            DETAILED_LOG_FILE.write("-" * 80 + "\n")
            if round_key:
                DETAILED_LOG_FILE.write(f"âš–ï¸ è£åˆ¤æ¨¡å‹ - {model_name} ({profile}) - {round_key} - question_id: {question_id}\n")
            else:
                DETAILED_LOG_FILE.write(f"âš–ï¸ è£åˆ¤æ¨¡å‹ - {model_name} ({profile}) - question_id: {question_id}\n")
            DETAILED_LOG_FILE.write(f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            DETAILED_LOG_FILE.write("-" * 80 + "\n")
            
            # è®°å½•è¯„åˆ¤ä¿¡æ¯
            DETAILED_LOG_FILE.write(f"æ¨¡å‹ç­”æ¡ˆ: {model_answer}\n")
            DETAILED_LOG_FILE.write(f"æ ‡å‡†ç­”æ¡ˆ: {gt_answer}\n")
            DETAILED_LOG_FILE.write(f"è¯„åˆ¤ç»“æœ: {'âœ… ä¸€è‡´' if is_match else 'âŒ ä¸ä¸€è‡´'}\n")
            DETAILED_LOG_FILE.write(f"è¯„åˆ¤ç†ç”±: {reasoning}\n")
            DETAILED_LOG_FILE.write(f"è€—æ—¶: {judge_time:.2f}ç§’\n")
            DETAILED_LOG_FILE.write("-" * 80 + "\n")
            
            # è®°å½•å®Œæ•´çš„æœ€ç»ˆæç¤ºè¯
            if prompt:
                DETAILED_LOG_FILE.write("ğŸ“‹ æœ€ç»ˆæäº¤ç»™è£åˆ¤æ¨¡å‹çš„å®Œæ•´æç¤ºè¯:\n")
                DETAILED_LOG_FILE.write("-" * 80 + "\n")
                DETAILED_LOG_FILE.write(prompt)
                DETAILED_LOG_FILE.write("\n")
                DETAILED_LOG_FILE.write("-" * 80 + "\n")
            
            # è®°å½•å®Œæ•´å“åº”å¯¹è±¡
            if raw_response:
                DETAILED_LOG_FILE.write("å®Œæ•´å“åº”å¯¹è±¡:\n")
                DETAILED_LOG_FILE.write(json.dumps(raw_response, indent=2, ensure_ascii=False, default=str))
                DETAILED_LOG_FILE.write("\n")
            else:
                DETAILED_LOG_FILE.write("âš ï¸ æ— åŸå§‹å“åº”å¯¹è±¡\n")
            
            DETAILED_LOG_FILE.write("=" * 80 + "\n\n")
            DETAILED_LOG_FILE.flush()
        except Exception as e:
            logging.warning(f"å†™å…¥è£åˆ¤æ¨¡å‹è¯¦ç»†æ—¥å¿—å¤±è´¥: {e}")

def setup_logging(log_dir: str, log_level: str = "INFO", log_mode: str = "detailed"):
    """
    é…ç½®æ—¥å¿—è®°å½•å™¨
    
    Args:
        log_dir: æ—¥å¿—ç›®å½•
        log_level: æ—¥å¿—çº§åˆ«ï¼ˆDEBUG/INFO/WARNING/ERRORï¼‰
        log_mode: æ—¥å¿—æ¨¡å¼ï¼ˆsimple/detailedï¼‰
    """
    global DETAILED_LOG_FILE, LOG_MODE
    
    LOG_MODE = log_mode.lower()
    log_dir = Path(log_dir)
    log_dir.mkdir(exist_ok=True)
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    log_file = log_dir / f'eval_{timestamp}.log'
    
    # æ ¹æ®æ—¥å¿—æ¨¡å¼é€‰æ‹©ä¸åŒçš„æ ¼å¼
    if LOG_MODE == "simple":
        # ç®€åŒ–æ¨¡å¼ï¼šåªæ˜¾ç¤ºçº§åˆ«å’Œæ¶ˆæ¯
        log_format = '%(levelname)s - %(message)s'
    else:
        # è¯¦ç»†æ¨¡å¼ï¼šæ˜¾ç¤ºæ—¶é—´ã€çº§åˆ«å’Œæ¶ˆæ¯
        log_format = '%(asctime)s - %(levelname)s - %(message)s'
        # æ‰“å¼€è¯¦ç»†æ—¥å¿—æ–‡ä»¶ï¼ˆç”¨äºè®°å½•å®Œæ•´å“åº”ï¼‰
        DETAILED_LOG_FILE = open(log_file, 'w', encoding='utf-8')
        DETAILED_LOG_FILE.write("=" * 80 + "\n")
        DETAILED_LOG_FILE.write("ğŸ“‹ è¯„æµ‹è¯¦ç»†æ—¥å¿—\n")
        DETAILED_LOG_FILE.write("=" * 80 + "\n")
        DETAILED_LOG_FILE.write(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        DETAILED_LOG_FILE.write(f"æ—¥å¿—æ¨¡å¼: {log_mode}\n")
        DETAILED_LOG_FILE.write("=" * 80 + "\n\n")
        DETAILED_LOG_FILE.flush()
    
    logging.basicConfig(
        level=getattr(logging, log_level.upper(), logging.INFO),
        format=log_format,
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler(str(log_file), encoding='utf-8')
        ],
        force=True
    )
    logging.info(f"æ—¥å¿—è®°å½•å™¨åˆå§‹åŒ–æˆåŠŸ (æ¨¡å¼: {log_mode})")


def evaluate_single_item(
    item: Dict[str, Any],
    enabled_models: List[str],
    profiles: List[str],
    workers: int = 1
) -> Optional[Dict[str, Any]]:
    """
    è¯„æµ‹å•ä¸ªæ•°æ®é¡¹
    
    Args:
        item: æ•°æ®é¡¹
        enabled_models: å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨
        profiles: ç”¨æˆ·ç”»åƒåˆ—è¡¨
        
    Returns:
        è¯„æµ‹ç»“æœå­—å…¸ï¼Œå¦‚æœå¤±è´¥è¿”å› None
    """
    # è·å–é—®é¢˜ID
    item_id = item.get("question_id") or item.get("id", "")
    
    logging.info(f"\n{'='*60}")
    logging.info(f"è¯„æµ‹æ•°æ®é¡¹: {item_id}")
    logging.info(f"{'='*60}")
    
    try:
        # è·å–æ•°æ®é¡¹ä¿¡æ¯
        question = item.get("question", "")
        answer = item.get("answer", "")
        options = item.get("options")
        
        # å¤„ç†image_pathï¼šæ”¯æŒå¤šå¼ å›¾ç‰‡ï¼ˆå¯èƒ½æ˜¯å­—ç¬¦ä¸²ã€åˆ—è¡¨æˆ–é€—å·åˆ†éš”çš„å­—ç¬¦ä¸²ï¼‰
        image_path_raw = item.get("image_path", "")
        image_paths = []
        
        if image_path_raw:
            if isinstance(image_path_raw, list):
                # å·²ç»æ˜¯åˆ—è¡¨æ ¼å¼
                image_paths = image_path_raw
            elif isinstance(image_path_raw, str):
                # å­—ç¬¦ä¸²æ ¼å¼ï¼šå¯èƒ½æ˜¯å•ä¸ªè·¯å¾„æˆ–é€—å·åˆ†éš”çš„å¤šä¸ªè·¯å¾„
                if ',' in image_path_raw:
                    # é€—å·åˆ†éš”çš„å¤šä¸ªè·¯å¾„
                    image_paths = [path.strip() for path in image_path_raw.split(',') if path.strip()]
                else:
                    # å•ä¸ªè·¯å¾„
                    image_paths = [image_path_raw] if image_path_raw.strip() else []
            else:
                # å…¶ä»–ç±»å‹ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²
                image_paths = [str(image_path_raw)] if image_path_raw else []
        
        # æ”¯æŒimage_urlså­—æ®µï¼ˆç¬¬äºŒç§æ ¼å¼ï¼‰
        image_urls = item.get("image_urls", [])
        if image_urls:
            # å¦‚æœæ˜¯URLï¼Œä¹Ÿæ·»åŠ åˆ°è·¯å¾„åˆ—è¡¨ï¼ˆAPIè°ƒç”¨æ—¶ä¼šå¤„ç†ï¼‰
            if isinstance(image_urls, list):
                image_paths.extend(image_urls)
            else:
                image_paths.append(image_urls)
        
        # å»é‡å¹¶è¿‡æ»¤ç©ºå€¼
        image_paths = [path for path in image_paths if path and path.strip()]
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºå¤šè½®é—®ç­”æ ¼å¼
        is_multi_round = item.get("is_multi_round", False)
        if not is_multi_round:
            # æ£€æŸ¥æ˜¯å¦ä¸ºå¤šè½®æ ¼å¼ï¼ˆquestionå’Œansweréƒ½æ˜¯å­—å…¸ï¼‰
            is_multi_round = (
                isinstance(question, dict) and 
                isinstance(answer, dict) and
                any(key.startswith("round") for key in question.keys())
            )
        
        has_options = options is not None and isinstance(options, dict) and any(options.values()) if options else False
        
        # è·å–å¹¶æ ‡å‡†åŒ–é¢˜å‹ï¼ˆç»Ÿä¸€è½¬æ¢ä¸ºä¸­æ–‡ï¼‰
        raw_question_type = item.get("question_type", "")
        normalized_question_type = ""
        if raw_question_type:
            try:
                from evaluate.prompts import normalize_question_type
                normalized_question_type = normalize_question_type(raw_question_type)
            except ImportError:
                from prompts import normalize_question_type
                normalized_question_type = normalize_question_type(raw_question_type)
        
        # å­˜å‚¨æ‰€æœ‰è¯„æµ‹ç»“æœï¼ˆä½¿ç”¨æ ‡å‡†æ ¼å¼å­—æ®µåï¼Œé¢˜å‹ä½¿ç”¨ä¸­æ–‡ï¼‰
        results = {
            "question_id": item_id,
            "image_id": item.get("image_id", ""),
            "image_path": image_paths[0] if image_paths else "",  # ä¿å­˜ç¬¬ä¸€ä¸ªè·¯å¾„ç”¨äºæ˜¾ç¤º
            "image_paths": image_paths,  # ä¿å­˜æ‰€æœ‰å›¾ç‰‡è·¯å¾„
            "image_type": item.get("image_type", ""),
            "question_type": normalized_question_type or raw_question_type,  # ä½¿ç”¨ä¸­æ–‡é¢˜å‹
            "question": question,
            "answer": answer,
            "options": options,
            "is_multi_round": is_multi_round,
            "profiles": {}
        }
        
        # ä¿ç•™åˆ†ç±»å­—æ®µï¼ˆç”¨äºç»Ÿè®¡ï¼‰
        for field in ["scenario", "capability", "difficulty", "source"]:
            if field in item:
                results[field] = item[field]
        
        # å¯¹æ¯ä¸ªç”¨æˆ·ç”»åƒè¿›è¡Œè¯„æµ‹
        for profile in profiles:
            logging.info(f"\n--- ç”¨æˆ·ç”»åƒ: {profile} ---")
            
            profile_results = {
                "profile": profile,
                "models": {}
            }
            
            # å¤„ç†å¤šè½®é—®ç­”
            if is_multi_round:
                # å¤šè½®é—®ç­”ï¼šä½¿ç”¨å¯¹è¯å†å²é€è½®è¯„æµ‹
                rounds_data = {}
                all_rounds_correct = True
                total_response_time = 0
                total_judge_time = 0
                
                # è·å–æ‰€æœ‰è½®æ¬¡ï¼ˆæŒ‰round1, round2...æ’åºï¼‰
                round_keys = sorted(
                    [k for k in question.keys() if k.startswith("round")],
                    key=lambda x: int(x.replace("round", "")) if x.replace("round", "").isdigit() else 999
                )
                
                # ä¸ºæ¯ä¸ªæ¨¡å‹ç»´æŠ¤å¯¹è¯å†å²ï¼ˆmessagesåˆ—è¡¨ï¼‰
                # æ ¼å¼ï¼š{model_name: [{"role": "user", "content": ...}, {"role": "assistant", "content": ...}, ...]}
                conversation_history = {model_name: [] for model_name in enabled_models}
                
                for round_key in round_keys:
                    round_num = round_key.replace("round", "")
                    round_question = question.get(round_key, "")
                    round_answer = answer.get(round_key, "")
                    
                    logging.info(f"  è½®æ¬¡ {round_num}: {round_question[:100]}...")
                    
                    # å¯¹æ¯ä¸ªå¯ç”¨çš„æ¨¡å‹è¿›è¡Œè¯„æµ‹
                    for model_name in enabled_models:
                        # model_name ç›´æ¥å¯¹åº” MODEL_DEFINITIONS ä¸­çš„ key
                        if round_key not in rounds_data:
                            rounds_data[round_key] = {}
                        
                        try:
                            # è·å–è¯¥ç”¨æˆ·ç”»åƒçš„æç¤ºè¯ï¼ˆå•è½®é—®é¢˜ï¼ŒåŒ…å«é¢˜å‹æç¤ºè¯ï¼‰
                            prompt = get_prompt(profile, round_question, None, normalized_question_type)
                            
                            # æ„å»ºå¯¹è¯å†å²ï¼šå¦‚æœæ˜¯ç¬¬ä¸€è½®ï¼ŒåªåŒ…å«å½“å‰é—®é¢˜ï¼›å¦åˆ™åŒ…å«å‰é¢çš„å¯¹è¯å†å²
                            messages = conversation_history[model_name].copy()
                            
                            # æ·»åŠ å½“å‰è½®æ¬¡çš„é—®é¢˜
                            # æ¯è½®éƒ½å¯ä»¥è¾“å…¥å›¾ç‰‡ï¼ˆå¦‚æœéœ€è¦ï¼‰ï¼Œä½†é€šå¸¸ç¬¬ä¸€è½®è¾“å…¥å³å¯
                            # æ£€æŸ¥å¯¹è¯å†å²ä¸­æ˜¯å¦å·²ç»åŒ…å«å›¾ç‰‡
                            has_image_in_history = False
                            if messages:
                                for msg in messages:
                                    if msg.get("role") == "user":
                                        content = msg.get("content", [])
                                        if isinstance(content, list):
                                            for item in content:
                                                if isinstance(item, dict) and item.get("type") == "image_url":
                                                    has_image_in_history = True
                                                    break
                                        if has_image_in_history:
                                            break
                            
                            # æ„å»ºå½“å‰è½®æ¬¡çš„useræ¶ˆæ¯
                            from evaluate.model_api import get_image_format, encode_image
                            user_content = []
                            
                            # å¦‚æœå¯¹è¯å†å²ä¸­æ²¡æœ‰å›¾ç‰‡ï¼Œä¸”å½“å‰æœ‰å›¾ç‰‡è·¯å¾„ï¼Œåˆ™æ·»åŠ å›¾ç‰‡
                            if not has_image_in_history and image_paths:
                                for image_path in image_paths:
                                    if image_path.startswith(("http://", "https://")):
                                        user_content.append({
                                            "type": "image_url",
                                            "image_url": {"url": image_path}
                                        })
                                    elif os.path.exists(image_path):
                                        image_format = get_image_format(image_path)
                                        base64_image = encode_image(image_path)
                                        user_content.append({
                                            "type": "image_url",
                                            "image_url": {"url": f"data:image/{image_format};base64,{base64_image}"}
                                        })
                            
                            # æ·»åŠ æ–‡æœ¬é—®é¢˜
                            user_content.append({"type": "text", "text": prompt})
                            current_user_msg = {"role": "user", "content": user_content}
                            
                            messages.append(current_user_msg)
                            
                            # è°ƒç”¨æ¨¡å‹APIï¼ˆä½¿ç”¨å¯¹è¯å†å²ï¼‰
                            model_answer, response_time, raw_response = call_model_api(
                                model_name=model_name,
                                messages=messages
                            )
                            
                            # æå–ç­”æ¡ˆï¼ˆç”¨äºæ·»åŠ åˆ°å¯¹è¯å†å²ï¼‰
                            extracted_answer, is_from_box, original_response = extract_answer_from_response(model_answer, False)
                            
                            # å°†æœ¬è½®é—®ç­”æ·»åŠ åˆ°å¯¹è¯å†å²ä¸­ï¼Œä¾›ä¸‹ä¸€è½®ä½¿ç”¨
                            # æ³¨æ„ï¼šassistantæ¶ˆæ¯åªä¿å­˜ç®€è¦ç­”æ¡ˆï¼ˆextracted_answerï¼‰ï¼Œè€Œä¸æ˜¯å®Œæ•´çš„å›ç­”ï¼Œä»¥å‡å°‘tokenæ¶ˆè€—
                            # å¦‚æœextracted_answerä¸ºç©ºï¼Œfallbackåˆ°original_responseçš„å500å­—ç¬¦ï¼ˆç­”æ¡ˆé€šå¸¸åœ¨ç»“å°¾ï¼‰
                            brief_answer_for_history = extracted_answer if extracted_answer and extracted_answer.strip() else (
                                original_response[-500:] if len(original_response) > 500 else original_response
                            )
                            
                            conversation_history[model_name].append(current_user_msg)
                            conversation_history[model_name].append({"role": "assistant", "content": brief_answer_for_history})
                            
                            # è¯¦ç»†æ—¥å¿—ï¼šè®°å½•æ¨¡å‹å“åº”
                            # å°†å¯¹è¯å†å²è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼ï¼ˆç”¨äºæ—¥å¿—ï¼‰
                            prompt_for_log = json.dumps(messages, ensure_ascii=False, indent=2) if messages else prompt
                            if LOG_MODE == "detailed" and DETAILED_LOG_FILE:
                                log_model_response_detailed(
                                    question_id=item_id,
                                    round_key=round_key,
                                    model_name=model_name,
                                    profile=profile,
                                    prompt=prompt_for_log,
                                    raw_response=raw_response
                                )
                            
                            # å¦‚æœ box æ²¡æå–åˆ°ä¸œè¥¿ï¼Œä½¿ç”¨å®Œæ•´ content è¿›è¡Œè£åˆ¤æ¨¡å‹è¯„æµ‹
                            answer_for_judge = original_response if not is_from_box else extracted_answer
                            
                            total_response_time += response_time
                            
                            # ä½¿ç”¨è£åˆ¤æ¨¡å‹è¯„åˆ¤
                            is_correct, reasoning, judge_time, judge_response, judge_prompt = judge_answer(
                                model_answer=answer_for_judge,
                                gt_answer=round_answer,
                                question=round_question,
                                options=None
                            )
                            
                            # è¯¦ç»†æ—¥å¿—ï¼šè®°å½•è£åˆ¤æ¨¡å‹å“åº”
                            if LOG_MODE == "detailed" and DETAILED_LOG_FILE:
                                log_judge_response_detailed(
                                    question_id=item_id,
                                    round_key=round_key,
                                    model_name=model_name,
                                    profile=profile,
                                    model_answer=answer_for_judge,  # ä½¿ç”¨å®é™…ç”¨äºè¯„åˆ¤çš„ç­”æ¡ˆ
                                    gt_answer=round_answer,
                                    is_match=is_correct,
                                    reasoning=reasoning,
                                    judge_time=judge_time,
                                    raw_response=judge_response,
                                    prompt=judge_prompt
                                )
                            
                            total_judge_time += judge_time
                            
                            if not is_correct:
                                all_rounds_correct = False
                            
                            logging.info(f"    è½®æ¬¡{round_num} æ¨¡å‹{model_name}: {'âœ“' if is_correct else 'âœ—'}")
                            
                            # ä¿å­˜è¯¥è½®æ¬¡çš„ç»“æœ
                            # æ³¨æ„ï¼šä¸ºäº†å…¼å®¹ module2 æ ¼å¼ï¼Œæˆ‘ä»¬ä¿å­˜ model_answer ä½œä¸º processï¼Œextracted_answer ä½œä¸º answer
                            result_data = {
                                "model_name": model_name,
                                "prompt": prompt_for_log,  # ä¿å­˜å®Œæ•´çš„å¯¹è¯å†å²ï¼ˆJSONæ ¼å¼ï¼‰
                                "conversation_history": messages,  # ä¿å­˜å¯¹è¯å†å²ï¼ˆåˆ—è¡¨æ ¼å¼ï¼‰
                                "model_answer": model_answer,  # å®Œæ•´å›ç­”ï¼ˆä½œä¸º processï¼‰
                                "extracted_answer": extracted_answer,  # æå–çš„ç­”æ¡ˆï¼ˆä½œä¸º answerï¼‰
                                "is_from_box": is_from_box,  # æ˜¯å¦ä» box ä¸­æå–
                                "answer_for_judge": answer_for_judge,  # å®é™…ç”¨äºè¯„åˆ¤çš„ç­”æ¡ˆ
                                "is_correct": is_correct,
                                "reasoning": reasoning,
                                "response_time": response_time,
                                "judge_time": judge_time,
                            }
                            # åªåœ¨è¯¦ç»†æ¨¡å¼ä¸‹ä¿å­˜å®Œæ•´å“åº”
                            if LOG_MODE == "detailed":
                                result_data["raw_response"] = raw_response
                                result_data["judge_response"] = judge_response
                            
                            rounds_data[round_key][model_name] = result_data
                            
                        except Exception as e:
                            logging.error(f"    è½®æ¬¡{round_num} æ¨¡å‹{model_name} è¯„æµ‹å¤±è´¥: {e}")
                            all_rounds_correct = False
                            if round_key not in rounds_data:
                                rounds_data[round_key] = {}
                            rounds_data[round_key][model_name] = {
                                "model_name": model_name,
                                "error": str(e),
                                "is_correct": False
                            }
                
                # æ±‡æ€»æ¯ä¸ªæ¨¡å‹çš„æ‰€æœ‰è½®æ¬¡ç»“æœ
                for model_name in enabled_models:
                    model_rounds = []
                    model_all_correct = True
                    for round_key in round_keys:
                        if round_key in rounds_data and model_name in rounds_data[round_key]:
                            round_result = rounds_data[round_key][model_name]
                            model_rounds.append({
                                "round": round_key,
                                "question": question.get(round_key, ""),
                                "answer": answer.get(round_key, ""),
                                **round_result
                            })
                            if not round_result.get("is_correct", False):
                                model_all_correct = False
                    
                    profile_results["models"][model_name] = {
                        "model_name": model_name,
                        "is_multi_round": True,
                        "rounds": model_rounds,
                        "all_rounds_correct": model_all_correct,
                        "total_response_time": total_response_time,
                        "total_judge_time": total_judge_time,
                        "is_correct": model_all_correct  # æ‰€æœ‰è½®æ¬¡éƒ½æ­£ç¡®æ‰ç®—æ­£ç¡®
                    }
            
            else:
                # å•è½®é—®ç­”ï¼šåŸæœ‰é€»è¾‘
                # è·å–è¯¥ç”¨æˆ·ç”»åƒçš„æç¤ºè¯ï¼ˆåŒ…å«é¢˜å‹æç¤ºè¯ï¼‰
                prompt = get_prompt(profile, question, options, normalized_question_type)
                logging.debug(f"æç¤ºè¯: {prompt[:200]}...")
                
            # å¯¹æ¯ä¸ªå¯ç”¨çš„æ¨¡å‹è¿›è¡Œè¯„æµ‹ï¼ˆå¹¶è¡Œï¼‰
            def eval_single_model(model_name: str):
                logging.info(f"  æ¨¡å‹: {model_name}")
                try:
                    model_answer, response_time, raw_response = call_model_api(
                        model_name=model_name,
                        prompt=prompt,
                        image_paths=image_paths if image_paths else None
                    )
                    
                    if LOG_MODE == "detailed" and DETAILED_LOG_FILE:
                        log_model_response_detailed(
                            question_id=item_id,
                            model_name=model_name,
                            profile=profile,
                            prompt=prompt,
                            raw_response=raw_response
                        )
                    
                    extracted_answer, is_from_box, original_response = extract_answer_from_response(model_answer, has_options)
                    answer_for_judge = original_response if not is_from_box else extracted_answer
                    
                    logging.info(f"    æ¨¡å‹å›ç­”: {extracted_answer[:100]}...")
                    if not is_from_box:
                        logging.info(f"    æ³¨æ„: æœªä» \\boxed{{}} ä¸­æå–åˆ°ç­”æ¡ˆï¼Œä½¿ç”¨å®Œæ•´å“åº”è¿›è¡Œè¯„æµ‹")
                    logging.info(f"    å“åº”æ—¶é—´: {response_time:.2f}s")
                    
                    is_correct, reasoning, judge_time, judge_response, judge_prompt = judge_answer(
                        model_answer=answer_for_judge,
                        gt_answer=answer,
                        question=question,
                        options=options
                    )
                    
                    if LOG_MODE == "detailed" and DETAILED_LOG_FILE:
                        log_judge_response_detailed(
                            question_id=item_id,
                            model_name=model_name,
                            profile=profile,
                            model_answer=answer_for_judge,
                            gt_answer=answer,
                            is_match=is_correct,
                            reasoning=reasoning,
                            judge_time=judge_time,
                            raw_response=judge_response,
                            prompt=judge_prompt
                        )
                    
                    logging.info(f"    è¯„åˆ¤ç»“æœ: {'âœ“' if is_correct else 'âœ—'} ({reasoning[:50]}...)")
                    logging.info(f"    è¯„åˆ¤æ—¶é—´: {judge_time:.2f}s")
                    
                    result_data = {
                        "model_name": model_name,
                        "prompt": prompt,
                        "model_answer": model_answer,
                        "extracted_answer": extracted_answer,
                        "is_from_box": is_from_box,
                        "answer_for_judge": answer_for_judge,
                        "is_correct": is_correct,
                        "reasoning": reasoning,
                        "response_time": response_time,
                        "judge_time": judge_time,
                    }
                    if LOG_MODE == "detailed":
                        result_data["raw_response"] = raw_response
                        result_data["judge_response"] = judge_response
                    return model_name, result_data
                except Exception as e:
                    logging.error(f"    æ¨¡å‹ {model_name} è¯„æµ‹å¤±è´¥: {e}")
                    return model_name, {
                        "model_name": model_name,
                        "error": str(e),
                        "is_correct": False
                    }

            with ThreadPoolExecutor(max_workers=max(1, workers)) as executor:
                futures = [executor.submit(eval_single_model, m) for m in enabled_models]
                for future in as_completed(futures):
                    model_name, model_result = future.result()
                    profile_results["models"][model_name] = model_result
            
            results["profiles"][profile] = profile_results
        
        return results
        
    except Exception as e:
        logging.error(f"æ•°æ®é¡¹ {item_id} è¯„æµ‹å¤±è´¥: {e}")
        return {"question_id": item_id, "error": str(e)}


def calculate_statistics(results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    
    Args:
        results: è¯„æµ‹ç»“æœåˆ—è¡¨
        
    Returns:
        ç»Ÿè®¡ä¿¡æ¯å­—å…¸
    """
    stats = {
        "total_items": len(results),
        "profiles": {},
        "models": {}
    }
    
    # è·å–æ‰€æœ‰ç”¨æˆ·ç”»åƒå’Œæ¨¡å‹
    all_profiles = set()
    all_models = set()
    
    for result in results:
        for profile in result.get("profiles", {}).keys():
            all_profiles.add(profile)
            for model_name in result["profiles"][profile].get("models", {}).keys():
                all_models.add(model_name)
    
    # æŒ‰ç”¨æˆ·ç”»åƒç»Ÿè®¡
    for profile in all_profiles:
        profile_stats = {
            "total": 0,
            "correct": 0,
            "models": {}
        }
        
        for model_name in all_models:
            model_stats = {"total": 0, "correct": 0}
            
            for result in results:
                profile_data = result.get("profiles", {}).get(profile, {})
                model_data = profile_data.get("models", {}).get(model_name, {})
                
                # æ”¯æŒå¤šè½®é—®ç­”æ ¼å¼
                is_correct = None
                if "is_correct" in model_data:
                    is_correct = model_data["is_correct"]
                elif "all_rounds_correct" in model_data:
                    is_correct = model_data["all_rounds_correct"]
                
                if is_correct is not None:
                    model_stats["total"] += 1
                    if is_correct:
                        model_stats["correct"] += 1
                        profile_stats["correct"] += 1
                    profile_stats["total"] += 1
            
            model_stats["accuracy"] = model_stats["correct"] / model_stats["total"] if model_stats["total"] > 0 else 0
            profile_stats["models"][model_name] = model_stats
        
        profile_stats["accuracy"] = profile_stats["correct"] / profile_stats["total"] if profile_stats["total"] > 0 else 0
        stats["profiles"][profile] = profile_stats
    
    # æŒ‰æ¨¡å‹ç»Ÿè®¡ï¼ˆè·¨æ‰€æœ‰ç”¨æˆ·ç”»åƒï¼‰
    for model_name in all_models:
        model_stats = {"total": 0, "correct": 0, "profiles": {}}
        
        for profile in all_profiles:
            profile_model_stats = {"total": 0, "correct": 0}
            
            for result in results:
                profile_data = result.get("profiles", {}).get(profile, {})
                model_data = profile_data.get("models", {}).get(model_name, {})
                
                # æ”¯æŒå¤šè½®é—®ç­”æ ¼å¼
                is_correct = None
                if "is_correct" in model_data:
                    is_correct = model_data["is_correct"]
                elif "all_rounds_correct" in model_data:
                    is_correct = model_data["all_rounds_correct"]
                
                if is_correct is not None:
                    profile_model_stats["total"] += 1
                    model_stats["total"] += 1
                    if is_correct:
                        profile_model_stats["correct"] += 1
                        model_stats["correct"] += 1
            
            profile_model_stats["accuracy"] = profile_model_stats["correct"] / profile_model_stats["total"] if profile_model_stats["total"] > 0 else 0
            model_stats["profiles"][profile] = profile_model_stats
        
        model_stats["accuracy"] = model_stats["correct"] / model_stats["total"] if model_stats["total"] > 0 else 0
        stats["models"][model_name] = model_stats
    
    # æŒ‰åˆ†ç±»å­—æ®µç»Ÿè®¡
    category_fields = ["question_type", "scenario", "capability", "difficulty", "source"]
    stats["by_category"] = {}
    
    for category_field in category_fields:
        category_stats = {}
        
        # æ”¶é›†æ‰€æœ‰è¯¥å­—æ®µçš„å€¼
        category_values = set()
        for result in results:
            category_value = result.get(category_field)
            if category_value:
                category_values.add(category_value)
        
        # å¯¹æ¯ä¸ªåˆ†ç±»å€¼è¿›è¡Œç»Ÿè®¡
        for category_value in category_values:
            category_value_stats = {
                "total": 0,
                "correct": 0,
                "models": {}
            }
            
            for model_name in all_models:
                model_category_stats = {"total": 0, "correct": 0}
                
                for result in results:
                    if result.get(category_field) != category_value:
                        continue
                    
                    # ç»Ÿè®¡æ‰€æœ‰ç”¨æˆ·ç”»åƒçš„ç»“æœ
                    for profile in all_profiles:
                        profile_data = result.get("profiles", {}).get(profile, {})
                        model_data = profile_data.get("models", {}).get(model_name, {})
                        
                        # æ”¯æŒå¤šè½®é—®ç­”æ ¼å¼
                        is_correct = None
                        if "is_correct" in model_data:
                            is_correct = model_data["is_correct"]
                        elif "all_rounds_correct" in model_data:
                            is_correct = model_data["all_rounds_correct"]
                        
                        if is_correct is not None:
                            model_category_stats["total"] += 1
                            category_value_stats["total"] += 1
                            if is_correct:
                                model_category_stats["correct"] += 1
                                category_value_stats["correct"] += 1
                
                model_category_stats["accuracy"] = model_category_stats["correct"] / model_category_stats["total"] if model_category_stats["total"] > 0 else 0
                category_value_stats["models"][model_name] = model_category_stats
            
            category_value_stats["accuracy"] = category_value_stats["correct"] / category_value_stats["total"] if category_value_stats["total"] > 0 else 0
            category_stats[category_value] = category_value_stats
        
        if category_stats:
            stats["by_category"][category_field] = category_stats
    
    return stats


def calculate_output_statistics(results: List[Dict[str, Any]], enabled_models: List[str]) -> Dict[str, Any]:
    """
    åŸºäºæœ€ç»ˆè¾“å‡ºæ ¼å¼è®¡ç®—ç»Ÿè®¡ä¿¡æ¯ï¼ˆç”¨äºåœ¨è¾“å‡ºæ–‡ä»¶ä¸­å±•ç¤ºï¼‰
    
    Args:
        results: module2æ ¼å¼çš„ç»“æœåˆ—è¡¨ï¼ˆæ¯ä¸ªç»“æœé¡¹å¯èƒ½åŒ…å«å¤šä¸ªæ¨¡å‹å­—æ®µï¼‰
        enabled_models: å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨
        
    Returns:
        ç»Ÿè®¡ä¿¡æ¯å­—å…¸ï¼ŒåŒ…å«æ€»å¾—åˆ†å’ŒæŒ‰åˆ†ç±»å­—æ®µçš„å¾—åˆ†
    """
    def _model_entry_is_valid(entry: Any) -> bool:
        """åˆ¤å®šæ¨¡å‹å­—æ®µæ˜¯å¦åŒ…å«æœ‰æ•ˆç»“æœï¼Œè¿‡æ»¤æ‰æ‰¹é‡å†™å…¥æ—¶ç”Ÿæˆçš„å ä½ç©ºç»“æœã€‚"""
        if not isinstance(entry, dict):
            return False
        if entry.get("response_time", 0) > 0:
            return True
        if entry.get("answer") not in ("", None, {}):
            return True
        if entry.get("process") not in ("", None, {}):
            return True
        return False
    
    model_keys = [f"model{i+1}" for i in range(len(enabled_models))]
    
    stats = {
        "total": {
            "total_count": 0,
            "correct_count": 0,
            "accuracy": 0.0
        },
        "by_model": {},
        "by_profile": {},
        "by_category": {}
    }
    
    # ç»Ÿè®¡æ€»å¾—åˆ†ï¼ˆé€æ¨¡å‹è€Œä¸æ˜¯é€é¢˜ï¼‰ï¼Œè·³è¿‡å ä½ç©ºç»“æœ
    total_correct = 0
    total_count = 0
    for item in results:
        for idx, _ in enumerate(enabled_models):
            model_key = model_keys[idx]
            entry = item.get(model_key)
            if not _model_entry_is_valid(entry):
                continue
            total_count += 1
            if entry.get("match_gt", False):
                total_correct += 1
    
    stats["total"]["total_count"] = total_count
    stats["total"]["correct_count"] = total_correct
    stats["total"]["accuracy"] = total_correct / total_count if total_count > 0 else 0.0
    
    # æŒ‰æ¨¡å‹ç»Ÿè®¡
    for idx, model_name in enumerate(enabled_models):
        model_key = model_keys[idx]
        model_total = 0
        model_correct = 0
        
        for item in results:
            entry = item.get(model_key)
            if not _model_entry_is_valid(entry):
                continue
            model_total += 1
            if entry.get("match_gt", False):
                model_correct += 1
        
        stats["by_model"][model_name] = {
            "total_count": model_total,
            "correct_count": model_correct,
            "accuracy": model_correct / model_total if model_total > 0 else 0.0
        }
    
    # æŒ‰ç”¨æˆ·ç”»åƒç»Ÿè®¡
    profiles = {item["profile"] for item in results if "profile" in item}
    
    for profile in profiles:
        profile_total = 0
        profile_correct = 0
        
        for item in results:
            if item.get("profile") != profile:
                continue
            for idx, _ in enumerate(enabled_models):
                model_key = model_keys[idx]
                entry = item.get(model_key)
                if not _model_entry_is_valid(entry):
                    continue
                profile_total += 1
                if entry.get("match_gt", False):
                    profile_correct += 1
        
        stats["by_profile"][profile] = {
            "total_count": profile_total,
            "correct_count": profile_correct,
            "accuracy": profile_correct / profile_total if profile_total > 0 else 0.0
        }
    
    # æŒ‰åˆ†ç±»å­—æ®µç»Ÿè®¡ï¼ˆåªç»Ÿè®¡å®é™…å­˜åœ¨çš„å­—æ®µï¼‰
    category_fields = ["question_type", "scenario", "capability", "difficulty", "source"]
    
    # å…ˆæ£€æŸ¥å“ªäº›å­—æ®µå®é™…å­˜åœ¨
    existing_fields = set()
    for item in results:
        for field in category_fields:
            if field in item and item[field]:
                existing_fields.add(field)
    
    # åªç»Ÿè®¡å­˜åœ¨çš„å­—æ®µ
    for category_field in existing_fields:
        category_stats = {}
        
        # æ”¶é›†æ‰€æœ‰è¯¥å­—æ®µçš„å€¼
        category_values = set()
        for item in results:
            category_value = item.get(category_field)
            if category_value:
                category_values.add(category_value)
        
        # å¯¹æ¯ä¸ªåˆ†ç±»å€¼è¿›è¡Œç»Ÿè®¡
        for category_value in category_values:
            category_total = 0
            category_correct = 0
            
            for item in results:
                if item.get(category_field) != category_value:
                    continue
                
                for idx, _ in enumerate(enabled_models):
                    model_key = model_keys[idx]
                    entry = item.get(model_key)
                    if not _model_entry_is_valid(entry):
                        continue
                    category_total += 1
                    if entry.get("match_gt", False):
                        category_correct += 1
            
            category_stats[category_value] = {
                "total_count": category_total,
                "correct_count": category_correct,
                "accuracy": category_correct / category_total if category_total > 0 else 0.0
            }
        
        if category_stats:
            stats["by_category"][category_field] = category_stats
    
    return stats


def main(args: argparse.Namespace):
    """ä¸»å‡½æ•°"""
    global DETAILED_LOG_FILE
    # è®¾ç½®æ—¥å¿—ï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–æ—¥å¿—æ¨¡å¼ï¼‰
    log_mode = os.getenv("EVAL_LOG_MODE", "detailed")
    setup_logging(args.log_dir, args.log_level, log_mode)
    
    # åŠ è½½æ•°æ®
    logging.info(f"åŠ è½½æ•°æ®: {args.input_file}")
    items = load_and_validate(args.input_file)
    logging.info(f"æˆåŠŸåŠ è½½ {len(items)} æ¡æ•°æ®")
    
    # é™åˆ¶å¤„ç†æ•°é‡ï¼ˆå¦‚æœè®¾ç½®äº†LIMITç¯å¢ƒå˜é‡ï¼‰
    limit = os.getenv("EVAL_LIMIT", "")
    if limit and limit.isdigit():
        limit_num = int(limit)
        if limit_num > 0:
            # æ˜¯å¦éšæœºé€‰æ‹©
            use_random = os.getenv("EVAL_USE_RANDOM", "false").lower() in ("true", "1", "yes")
            if use_random:
                import random
                seed = os.getenv("EVAL_SEED", "")
                if seed and seed.isdigit():
                    random.seed(int(seed))
                random.shuffle(items)
            items = items[:limit_num]
            logging.info(f"é™åˆ¶å¤„ç†æ•°é‡: {limit_num} æ¡æ•°æ®")
    
    # ç¡®å®šè¦è¯„æµ‹çš„æ¨¡å‹å’Œç”¨æˆ·ç”»åƒ
    # ä»ç¯å¢ƒå˜é‡è¯»å–è¦è¯„æµ‹çš„æ¨¡å‹åˆ—è¡¨ï¼ˆæ¨¡å‹åç§°å¯¹åº” MODEL_DEFINITIONS ä¸­çš„ keyï¼‰
    eval_model_names = get_eval_models()
    
    if not eval_model_names:
        raise ValueError(
            "æ²¡æœ‰æŒ‡å®šè¦è¯„æµ‹çš„æ¨¡å‹ã€‚è¯·åœ¨è„šæœ¬ä¸­è®¾ç½® EVAL_MODELS ç¯å¢ƒå˜é‡ï¼Œ"
            "ä¾‹å¦‚ï¼šexport EVAL_MODELS='doubao,GLM,qwenvlmax'"
        )
    
    # éªŒè¯æ¨¡å‹æ˜¯å¦åœ¨ MODEL_DEFINITIONS ä¸­
    enabled_models = []
    for model_name in eval_model_names:
        if model_name in MODEL_DEFINITIONS:
            enabled_models.append(model_name)
        else:
            logging.warning(f"æ¨¡å‹ '{model_name}' ä¸åœ¨ MODEL_DEFINITIONS ä¸­ï¼Œå·²è·³è¿‡")
    
    if not enabled_models:
        raise ValueError(f"æ²¡æœ‰æœ‰æ•ˆçš„æ¨¡å‹ã€‚è¯·æ£€æŸ¥ EVAL_MODELS ç¯å¢ƒå˜é‡å’Œ MODEL_DEFINITIONS é…ç½®")
    
    profiles = args.profiles if args.profiles else USER_PROFILES
    
    logging.info(f"å¯ç”¨çš„æ¨¡å‹: {enabled_models}")
    logging.info(f"ç”¨æˆ·ç”»åƒ: {profiles}")
    
    # æ£€æŸ¥ï¼šå¦‚æœå¯ç”¨æ–­ç‚¹ç»­ä¼ ä½†æ²¡æœ‰æŒ‡å®šè¾“å‡ºæ–‡ä»¶åï¼ŒæŠ¥é”™
    use_custom_output_file = args.output_file is not None and args.output_file != ""
    if args.resume and not use_custom_output_file:
        error_msg = (
            "é”™è¯¯ï¼šå¯ç”¨æ–­ç‚¹ç»­ä¼ æ—¶å¿…é¡»æŒ‡å®šè¾“å‡ºæ–‡ä»¶åï¼ˆOUTPUT_FILEï¼‰ã€‚\n"
            "è¯·åœ¨ run_eval.sh ä¸­è®¾ç½® OUTPUT_FILEï¼Œä¾‹å¦‚ï¼šOUTPUT_FILE=\"eval_results.json\""
        )
        logging.error(error_msg)
        raise ValueError(error_msg)
    
    # è¾“å‡ºç›®å½•å›ºå®šä¸º ./outputsï¼ŒæŒ‰ç”¨æˆ·ç”»åƒå’Œæ¨¡å‹åˆ†ç±»ç»„ç»‡
    base_output_dir = Path("./outputs")
    base_output_dir.mkdir(exist_ok=True)
    
    if use_custom_output_file:
        # è§£ææŒ‡å®šçš„è¾“å‡ºæ–‡ä»¶å
        output_file_path = Path(args.output_file)
        # åªä½¿ç”¨æ–‡ä»¶åéƒ¨åˆ†ï¼Œå¿½ç•¥è·¯å¾„ï¼ˆå› ä¸ºè·¯å¾„ç”± profile å’Œ model_name å†³å®šï¼‰
        output_file_name = output_file_path.name
        base_name = output_file_path.stem
        file_ext = output_file_path.suffix.lstrip('.')
        
        # å¦‚æœæ–‡ä»¶æœ‰æ‰©å±•åï¼Œä½¿ç”¨æ‰©å±•åï¼›å¦åˆ™ä½¿ç”¨é…ç½®ä¸­çš„æ ¼å¼
        if file_ext:
            output_format = file_ext.lower()
        else:
            output_format = EVAL_CONFIG.get("output_format", "json").lower()
            file_ext = output_format
            output_file_name = f"{base_name}.{file_ext}"
        
        logging.info(f"ä½¿ç”¨æŒ‡å®šçš„è¾“å‡ºæ–‡ä»¶å: {output_file_name}")
        logging.info(f"è¾“å‡ºæ ¼å¼: {output_format}")
        logging.info(f"æ–‡ä»¶å°†ä¿å­˜åœ¨: ./outputs/{{profile}}/{{model_name}}/{output_file_name}")
    else:
        # ä½¿ç”¨è‡ªåŠ¨ç”Ÿæˆçš„å¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶åï¼ˆä»…åœ¨ä¸æ–­ç‚¹ç»­ä¼ æ—¶ï¼‰
        # æ³¨æ„ï¼šå¦‚æœå¯ç”¨æ–­ç‚¹ç»­ä¼ ï¼Œåº”è¯¥å·²ç»åœ¨ä¸Šé¢çš„æ£€æŸ¥ä¸­æŠ¥é”™äº†
        input_file_name = Path(args.input_file).stem  # è¯„æµ‹é›†å‘½åï¼ˆä¸å«æ‰©å±•åï¼‰
        limit_str = str(len(items)) if limit and limit.isdigit() and int(limit) > 0 else "all"
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_format = EVAL_CONFIG.get("output_format", "json").lower()
        output_file_name = f"eval_{input_file_name}_{limit_str}_{timestamp}.{output_format}"
        
        logging.info(f"ä½¿ç”¨è‡ªåŠ¨ç”Ÿæˆçš„è¾“å‡ºæ–‡ä»¶å: {output_file_name}")
        logging.info(f"è¾“å‡ºæ ¼å¼: {output_format}")
        logging.info(f"æ–‡ä»¶å°†ä¿å­˜åœ¨: ./outputs/{{profile}}/{{model_name}}/{output_file_name}")
    
    # è¾…åŠ©å‡½æ•°ï¼šè·å–ä¸‹ä¸€ä¸ªç‰ˆæœ¬å·çš„æ–‡ä»¶è·¯å¾„ï¼ˆç±»ä¼¼ module1ï¼‰
    def get_next_version_path(original_path: Path) -> Path:
        """å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œç”Ÿæˆ_v2ã€_v3ç­‰ç‰ˆæœ¬å·çš„æ–‡ä»¶è·¯å¾„"""
        if not original_path.exists():
            return original_path
        
        base_name = original_path.stem
        ext = original_path.suffix
        dir_path = original_path.parent
        
        counter = 2
        while True:
            new_name = f"{base_name}_v{counter}{ext}"
            new_path = dir_path / new_name
            if not new_path.exists():
                return new_path
            counter += 1
    
    # ä¸ºæ¯ä¸ªæ¨¡å‹å’Œç”¨æˆ·ç”»åƒç»„åˆåˆ›å»ºè¾“å‡ºæ–‡ä»¶
    output_files = {}  # {(model_name, profile): file_path}
    output_file_handles = {}  # {(model_name, profile): file_handle}  # ä»…ç”¨äºJSONL
    completed_items = {}  # {(model_name, profile): set(question_ids)}
    existing_results = {}  # {(model_name, profile): list(results)}
    
    for model_name in enabled_models:
        for profile in profiles:
            # æ–‡ä»¶è·¯å¾„ï¼š./outputs/{profile}/{model_name}/{output_file_name}
            profile_model_dir = base_output_dir / profile / model_name
            profile_model_dir.mkdir(parents=True, exist_ok=True)
            
            # æ–­ç‚¹ç»­ä¼ ï¼šæ£€æŸ¥æ˜¯å¦å­˜åœ¨åŒ¹é…çš„è¾“å‡ºæ–‡ä»¶å¹¶è¯»å–å·²å®Œæˆçš„é—®é¢˜
            existing_file = None
            if args.resume:
                # æ–­ç‚¹ç»­ä¼ æ¨¡å¼ä¸‹ï¼Œå¿…é¡»æŒ‡å®šäº†è¾“å‡ºæ–‡ä»¶åï¼ˆå¦åˆ™åº”è¯¥å·²ç»åœ¨ä¸Šé¢çš„æ£€æŸ¥ä¸­æŠ¥é”™ï¼‰
                base_output_file = profile_model_dir / output_file_name
                if base_output_file.exists():
                    existing_file = base_output_file
                    output_file = base_output_file
                    logging.info(f"æ£€æµ‹åˆ°è¾“å‡ºæ–‡ä»¶: {base_output_file}")
                else:
                    # æ£€æŸ¥æ˜¯å¦æœ‰å¸¦ç‰ˆæœ¬å·çš„æ–‡ä»¶ï¼ˆ_v2, _v3ç­‰ï¼‰
                    base_name_without_ext = Path(output_file_name).stem
                    pattern = f"{base_name_without_ext}_v*.{file_ext}"
                    versioned_files = list(profile_model_dir.glob(pattern))
                    if versioned_files:
                        # ä½¿ç”¨æœ€æ–°çš„ç‰ˆæœ¬å·æ–‡ä»¶
                        existing_file = max(versioned_files, key=lambda p: p.stat().st_mtime)
                        output_file = existing_file
                        logging.info(f"æ£€æµ‹åˆ°å¸¦ç‰ˆæœ¬å·çš„è¾“å‡ºæ–‡ä»¶: {existing_file}")
                    else:
                        # è¾“å‡ºæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè‡ªåŠ¨åˆ›å»º
                        output_file = base_output_file
                        logging.info(f"æ£€æµ‹åˆ°è¾“å‡ºæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†è‡ªåŠ¨åˆ›å»ºæ–°æ–‡ä»¶: {output_file}")
            else:
                # ä¸ç»­ä¼ 
                if use_custom_output_file:
                    # å¦‚æœæŒ‡å®šäº†è¾“å‡ºæ–‡ä»¶å
                    base_output_file = profile_model_dir / output_file_name
                    if base_output_file.exists():
                        output_file = get_next_version_path(base_output_file)
                        logging.info(f"æ–‡ä»¶å·²å­˜åœ¨ï¼Œä½¿ç”¨æ–°ç‰ˆæœ¬: {output_file}")
                    else:
                        output_file = base_output_file
                else:
                    # å¦‚æœæœªæŒ‡å®šè¾“å‡ºæ–‡ä»¶åï¼Œç”Ÿæˆæ–°çš„å¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶å
                    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                    output_file_name = f"eval_{input_file_name}_{limit_str}_{timestamp}.{output_format}"
                    output_file = profile_model_dir / output_file_name
            
            # å¦‚æœæ‰¾åˆ°äº†å·²å­˜åœ¨çš„æ–‡ä»¶ï¼Œè¯»å–å·²å®Œæˆçš„é—®é¢˜
            if existing_file and existing_file.exists():
                # éªŒè¯æ–‡ä»¶å…ƒæ•°æ®ï¼ˆæ£€æŸ¥è¾“å…¥æ–‡ä»¶è·¯å¾„æ˜¯å¦åŒ¹é…ï¼‰
                try:
                    if output_format == "jsonl":
                        # JSONLæ ¼å¼ï¼šè¯»å–ç¬¬ä¸€è¡Œï¼ˆç»Ÿè®¡ä¿¡æ¯ï¼‰å’Œå·²æœ‰ç»“æœ
                        with open(existing_file, 'r', encoding='utf-8') as f:
                            first_line = f.readline().strip()
                            if first_line:
                                stats_data = json.loads(first_line)
                                # æ£€æŸ¥è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœä¿å­˜äº†çš„è¯ï¼‰
                                # è¿™é‡Œå¯ä»¥æ‰©å±•æ£€æŸ¥é€»è¾‘
                                pass
                            
                            # è¯»å–å·²æœ‰ç»“æœ
                            completed_ids = set()
                            results_list = []
                            for line in f:
                                line = line.strip()
                                if not line:
                                    continue
                                try:
                                    item = json.loads(line)
                                    item_id = item.get("question_id", "")
                                    if item_id:
                                        completed_ids.add(item_id)
                                        results_list.append(item)
                                except json.JSONDecodeError:
                                    continue
                            
                            completed_items[(model_name, profile)] = completed_ids
                            existing_results[(model_name, profile)] = results_list
                            output_file = existing_file
                            logging.info(f"  å·²åŠ è½½ {len(completed_ids)} ä¸ªå·²å®Œæˆçš„é—®é¢˜")
                    else:
                        # JSONæ ¼å¼ï¼šè¯»å–å®Œæ•´æ–‡ä»¶
                        with open(existing_file, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                            if isinstance(data, dict) and "results" in data:
                                # æ£€æŸ¥ç»Ÿè®¡ä¿¡æ¯ä¸­çš„å…ƒæ•°æ®
                                stats = data.get("statistics", {})
                                # è¿™é‡Œå¯ä»¥æ‰©å±•æ£€æŸ¥é€»è¾‘
                                
                                # è¯»å–å·²æœ‰ç»“æœ
                                results_list = data.get("results", [])
                                completed_ids = set()
                                for item in results_list:
                                    item_id = item.get("question_id", "")
                                    if item_id:
                                        completed_ids.add(item_id)
                                
                                completed_items[(model_name, profile)] = completed_ids
                                existing_results[(model_name, profile)] = results_list
                                output_file = existing_file
                                logging.info(f"  å·²åŠ è½½ {len(completed_ids)} ä¸ªå·²å®Œæˆçš„é—®é¢˜")
                except Exception as e:
                    logging.warning(f"åŠ è½½å·²æœ‰æ–‡ä»¶å¤±è´¥: {e}ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶")
                    # å¦‚æœåŠ è½½å¤±è´¥ï¼Œä½¿ç”¨æ–°æ–‡ä»¶è·¯å¾„ï¼ˆä¸è¦†ç›– existing_fileï¼‰
                    pass
            
            output_files[(model_name, profile)] = output_file
            
            # JSONLæ ¼å¼ï¼šæ‰“å¼€æ–‡ä»¶å¥æŸ„ï¼ˆè¿½åŠ æ¨¡å¼ï¼‰
            if output_format == "jsonl":
                file_handle = open(output_file, 'a', encoding='utf-8')
                output_file_handles[(model_name, profile)] = file_handle
                
                # å¦‚æœæ˜¯æ–°æ–‡ä»¶ï¼Œå†™å…¥ç»Ÿè®¡ä¿¡æ¯å ä½ç¬¦ï¼ˆç¬¬ä¸€è¡Œï¼‰
                if output_file.stat().st_size == 0:
                    stats_placeholder = {
                        "statistics": {
                            "total": {"total_count": 0, "correct_count": 0, "accuracy": 0.0},
                            "by_model": {},
                            "by_profile": {},
                            "by_category": {}
                        }
                    }
                    file_handle.write(json.dumps(stats_placeholder, ensure_ascii=False) + '\n')
                    file_handle.flush()
    
    # åˆå§‹åŒ–æ‰¹é‡å†™å…¥bufferï¼ˆä»…ç”¨äºJSONæ ¼å¼ï¼‰
    batch_size = EVAL_CONFIG.get("batch_size", 10)  # é»˜è®¤æ¯10æ¡ä¿å­˜ä¸€æ¬¡
    result_buffers = {}  # {(model_name, profile): list(results)}
    for model_name in enabled_models:
        for profile in profiles:
            result_buffers[(model_name, profile)] = existing_results.get((model_name, profile), [])
    
    # è¾…åŠ©å‡½æ•°ï¼šå°†ç»“æœè½¬æ¢ä¸ºmodule2æ ¼å¼å¹¶å†™å…¥
    def convert_and_save_result(result: Dict[str, Any], model_name: str, profile: str):
        """å°†å•ä¸ªè¯„æµ‹ç»“æœè½¬æ¢ä¸ºmodule2æ ¼å¼å¹¶ä¿å­˜"""
        profile_data = result.get("profiles", {}).get(profile, {})
        model_data = profile_data.get("models", {}).get(model_name, {})
        
        if not model_data:
            return None
        
        # ç¡®å®šæ¨¡å‹é”®
        model_key = None
        for idx, enabled_model in enumerate(enabled_models, 1):
            if enabled_model == model_name:
                model_key = f"model{idx}"
                break
        if not model_key:
            model_key = "model1"
        
        # è·å–æ¨¡å‹ç­”æ¡ˆå’Œæ¨ç†è¿‡ç¨‹
        model_answer = model_data.get("model_answer", "")
        extracted_answer = model_data.get("extracted_answer", "")
        is_multi_round = result.get("is_multi_round", False)
        
        # å¤„ç†å¤šè½®é—®ç­”
        if is_multi_round and isinstance(model_data.get("rounds"), list):
            answer_dict = {}
            process_dict = {}
            for round_data in model_data.get("rounds", []):
                round_key = round_data.get("round", "")
                if round_key:
                    answer_dict[round_key] = round_data.get("extracted_answer", "")
                    process_dict[round_key] = round_data.get("model_answer", "")
            model_answer_value = answer_dict if answer_dict else {}
            process_value = process_dict if process_dict else {}
        else:
            model_answer_value = extracted_answer if extracted_answer else ""
            process_value = model_answer if model_answer else ""
        
        is_correct = model_data.get("is_correct", False) or model_data.get("all_rounds_correct", False)
        
        # æ„å»ºmodule2æ ¼å¼çš„ç»“æœé¡¹
        module2_item = {
            "question_id": result.get("question_id", result.get("id", "")),
            "question": result.get("question", ""),
            "answer": result.get("answer", ""),
            "question_type": result.get("question_type", ""),
            "image_type": result.get("image_type", ""),
            "image_path": result.get("image_path", ""),
            "options": result.get("options"),
            "profile": profile,
        }
        
        # ä¿ç•™åˆ†ç±»å­—æ®µ
        for field in ["scenario", "capability", "difficulty", "source"]:
            if field in result:
                module2_item[field] = result[field]
        
        # æ·»åŠ æ¨¡å‹ç»“æœ
        module2_item[model_key] = {
            "process": process_value,
            "answer": model_answer_value,
            "model_name": model_name,
            "response_time": model_data.get("response_time", 0.0),
            "match_gt": is_correct
        }
        
        # æ·»åŠ å…¶ä»–æ¨¡å‹å­—æ®µ
        for idx, other_model in enumerate(enabled_models, 1):
            other_model_key = f"model{idx}"
            if other_model_key != model_key:
                module2_item[other_model_key] = {
                    "process": "" if not is_multi_round else {},
                    "answer": "" if not is_multi_round else {},
                    "model_name": other_model,
                    "response_time": 0.0,
                    "match_gt": False
                }
        
        # æ·»åŠ comparisonå­—æ®µ
        module2_item["comparison"] = {
            "agreement_with_gt": 1 if is_correct else 0
        }
        
        return module2_item
    
    # è¾…åŠ©å‡½æ•°ï¼šæ‰¹é‡å†™å…¥JSONæ ¼å¼ç»“æœ
    def flush_buffer(model_name: str, profile: str):
        """åˆ·æ–°æŒ‡å®šæ¨¡å‹å’Œç”¨æˆ·ç”»åƒçš„buffer"""
        key = (model_name, profile)
        if key not in result_buffers:
            return
        
        buffer = result_buffers[key]
        if not buffer:
            return
        
        output_file = output_files[key]
        
        try:
            # è¯»å–ç°æœ‰æ•°æ®
            existing_data = {"statistics": {}, "results": []}
            if output_file.exists() and output_file.stat().st_size > 0:
                with open(output_file, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
            
            # åˆå¹¶ç»“æœï¼ˆå»é‡ï¼‰
            existing_ids = {item.get("question_id", "") for item in existing_data.get("results", [])}
            new_results = []
            for item in buffer:
                item_id = item.get("question_id", "")
                if item_id and item_id not in existing_ids:
                    new_results.append(item)
                    existing_ids.add(item_id)
            
            if new_results:
                existing_data["results"].extend(new_results)
                
                # é‡æ–°è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
                stats = calculate_output_statistics(existing_data["results"], enabled_models)
                existing_data["statistics"] = stats
                
                # ä¿å­˜
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(existing_data, f, ensure_ascii=False, indent=2)
                
                logging.debug(f"æ‰¹é‡ä¿å­˜ {len(new_results)} æ¡ç»“æœåˆ° {output_file.name}")
            
            # æ¸…ç©ºbufferï¼ˆåªä¿ç•™å·²ä¿å­˜çš„ç»“æœç”¨äºå»é‡æ£€æŸ¥ï¼‰
            result_buffers[key] = existing_data["results"]
        except Exception as e:
            logging.error(f"æ‰¹é‡ä¿å­˜å¤±è´¥ ({model_name}, {profile}): {e}")
    
    # å¹¶å‘é…ç½®
    workers = os.getenv("EVAL_WORKERS", "")
    try:
        workers = int(workers) if str(workers).strip() else 1
    except ValueError:
        workers = 1
    if workers <= 0:
        workers = 1
    separator = "=" * 40
    combos = [(m, p) for m in enabled_models for p in profiles]
    combo_workers = max(1, min(len(combos), workers))
    per_combo_workers = max(1, workers // combo_workers)
    logging.info(
        f"å¹¶å‘é…ç½®: total_workers={workers}, combo_workers={combo_workers}, per_combo_workers={per_combo_workers}"
    )

    # è¯„æµ‹æ¯ä¸ªæ¨¡å‹/ç”¨æˆ·ç”»åƒç»„åˆï¼Œå¹¶è¡Œè°ƒåº¦ç»„åˆï¼Œåœ¨ç»„åˆå†…éƒ¨å†å¹¶å‘é¢˜ç›®ï¼ˆå¤šè½®è§†ä¸ºå•ä»»åŠ¡ï¼‰
    failures = []  # æ”¶é›†å¤±è´¥é—®é¢˜

    def process_combo(model_name: str, profile: str):
        key = (model_name, profile)
        logging.info(f"\n{separator}\nå¼€å§‹æ¨¡å‹: {model_name} | ç”»åƒ: {profile}\n{separator}")
        futures = {}
        try:
            with ThreadPoolExecutor(max_workers=per_combo_workers) as executor:
                for item in items:
                    item_id = item.get("question_id") or item.get("id", "")
                    if item_id and item_id in completed_items.get(key, set()):
                        continue
                    future = executor.submit(
                        evaluate_single_item,
                        item,
                        [model_name],  # å•æ¨¡å‹ï¼Œé¿å…å†…éƒ¨å†æ¬¡å¹¶è¡Œ
                        [profile],
                        1             # å†…éƒ¨ä¸å¼€çº¿ç¨‹æ± 
                    )
                    futures[future] = item_id
                
                # ä¸ºæ¯ä¸ªæ¨¡å‹/ç”»åƒç»„åˆå•ç‹¬æ˜¾ç¤ºä¸€ä¸ªè¿›åº¦æ¡ï¼Œå‰ç¼€åŒ…å«æ¨¡å‹åå’Œç”»åƒï¼Œä¾¿äºåŒºåˆ†
                for future in tqdm(
                    as_completed(futures),
                    total=len(futures),
                    desc=f"{model_name}-{profile}",
                ):
                    item_id = futures[future]
                    try:
                        result = future.result()
                    except Exception as e:
                        failures.append({"question_id": item_id, "reason": f"future exception: {e}"})
                        continue
                    
                    if not result:
                        failures.append({"question_id": item_id, "reason": "evaluate_single_item returned None"})
                        continue
                    if isinstance(result, dict) and "error" in result:
                        failures.append({"question_id": result.get("question_id", item_id), "reason": result.get("error", "unknown error")})
                        continue
                    
                    module2_item = convert_and_save_result(result, model_name, profile)
                    if not module2_item:
                        continue
                    
                    item_id = module2_item.get("question_id", "") or item_id
                    if not item_id:
                        continue
                    if item_id in completed_items.get(key, set()):
                        continue
                    
                    if key not in completed_items:
                        completed_items[key] = set()
                    completed_items[key].add(item_id)
                    
                    if output_format == "jsonl":
                        file_handle = output_file_handles.get(key)
                        if file_handle:
                            try:
                                file_handle.write(json.dumps(module2_item, ensure_ascii=False) + '\n')
                                file_handle.flush()
                            except Exception as e:
                                logging.error(f"å®æ—¶å†™å…¥å¤±è´¥ ({model_name}, {profile}): {e}")
                    else:
                        if key not in result_buffers:
                            result_buffers[key] = []
                        result_buffers[key].append(module2_item)
                        
                        if len(result_buffers[key]) >= batch_size:
                            flush_buffer(model_name, profile)
        finally:
            # ç¡®ä¿å½“å‰æ¨¡å‹ç”»åƒçš„ç¼“å†²è¢«åˆ·æ–°ï¼ˆå³ä½¿å¼‚å¸¸/ä¸­æ–­ï¼‰
            if output_format == "json":
                flush_buffer(model_name, profile)

    interrupted = False
    try:
        with ThreadPoolExecutor(max_workers=combo_workers) as combo_executor:
            combo_future_map = {combo_executor.submit(process_combo, m, p): (m, p) for m, p in combos}
            for future in as_completed(combo_future_map):
                m, p = combo_future_map[future]
                try:
                    future.result()
                except Exception as e:
                    failures.append({"question_id": "", "reason": f"combo {m}-{p} exception: {e}"})
    except KeyboardInterrupt:
        interrupted = True
        logging.warning("æ£€æµ‹åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨å°è¯•ä¼˜é›…åœæ­¢å¹¶åˆ·ç›˜...")
        try:
            combo_executor.shutdown(wait=False, cancel_futures=True)
        except Exception:
            pass

    # å…³é—­æ‰€æœ‰æ–‡ä»¶å¥æŸ„ï¼ˆJSONLæ ¼å¼ï¼Œåœ¨æ›´æ–°ç»Ÿè®¡ä¿¡æ¯å‰å…³é—­ï¼‰
    if output_format == "jsonl":
        for key, file_handle in output_file_handles.items():
            try:
                file_handle.close()
            except Exception as e:
                logging.warning(f"å…³é—­æ–‡ä»¶å¥æŸ„å¤±è´¥ {key}: {e}")
    
    # åˆ·æ–°æ‰€æœ‰bufferï¼ˆJSONæ ¼å¼ï¼‰
    if output_format == "json":
        logging.info("\nåˆ·æ–°æ‰€æœ‰buffer...")
        for model_name in enabled_models:
            for profile in profiles:
                flush_buffer(model_name, profile)
    
    # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯å¹¶ä¿å­˜æœ€ç»ˆç»“æœ
    logging.info("\næ›´æ–°ç»Ÿè®¡ä¿¡æ¯...")
    for model_name in enabled_models:
        for profile in profiles:
            key = (model_name, profile)
            output_file = output_files[key]
            
            try:
                if output_format == "jsonl":
                    # JSONLæ ¼å¼ï¼šè¯»å–æ‰€æœ‰ç»“æœï¼Œé‡æ–°è®¡ç®—ç»Ÿè®¡ä¿¡æ¯ï¼Œæ›´æ–°ç¬¬ä¸€è¡Œ
                    all_results = []
                    with open(output_file, 'r', encoding='utf-8') as f:
                        lines = f.readlines()
                        if lines:
                            # è·³è¿‡ç¬¬ä¸€è¡Œï¼ˆç»Ÿè®¡ä¿¡æ¯ï¼‰
                            for line in lines[1:]:
                                line = line.strip()
                                if line:
                                    try:
                                        all_results.append(json.loads(line))
                                    except json.JSONDecodeError:
                                        continue
                    
                    # é‡æ–°è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
                    stats = calculate_output_statistics(all_results, enabled_models)
                    
                    # é‡å†™æ–‡ä»¶ï¼ˆæ›´æ–°ç¬¬ä¸€è¡Œç»Ÿè®¡ä¿¡æ¯ï¼‰
                    with open(output_file, 'w', encoding='utf-8') as f:
                        f.write(json.dumps({"statistics": stats}, ensure_ascii=False) + '\n')
                        for item in all_results:
                            f.write(json.dumps(item, ensure_ascii=False) + '\n')
                    
                    logging.info(f"å·²æ›´æ–°ç»Ÿè®¡ä¿¡æ¯: {output_file.name} (å…± {len(all_results)} æ¡ç»“æœ)")
                else:
                    # JSONæ ¼å¼ï¼šé‡æ–°è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
                    with open(output_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    all_results = data.get("results", [])
                    stats = calculate_output_statistics(all_results, enabled_models)
                    data["statistics"] = stats
                    
                    with open(output_file, 'w', encoding='utf-8') as f:
                        json.dump(data, f, ensure_ascii=False, indent=2)
                    
                    logging.info(f"å·²æ›´æ–°ç»Ÿè®¡ä¿¡æ¯: {output_file.name} (å…± {len(all_results)} æ¡ç»“æœ)")
            except Exception as e:
                logging.error(f"æ›´æ–°ç»Ÿè®¡ä¿¡æ¯å¤±è´¥ ({model_name}, {profile}): {e}")
    
    # å¦‚æœæ˜¯ä¸­æ–­é€€å‡ºï¼Œä¿å­˜å®Œå°±ç«‹åˆ»é™é»˜é€€å‡ºï¼ˆä¸å†æ‰“å°åç»­æ‘˜è¦æ—¥å¿—ï¼‰
    if interrupted:
        logging.info(f"\nè¯„æµ‹ä¸­æ–­ï¼Œä½†å·²ä¿å­˜å½“å‰ç»“æœå¹¶æ›´æ–°ç»Ÿè®¡ï¼Œå…±ç”Ÿæˆ {len(output_files)} ä¸ªè¾“å‡ºæ–‡ä»¶ï¼Œå‡†å¤‡é™é»˜é€€å‡ºã€‚")
        # ç›´æ¥é€€å‡ºè¿›ç¨‹ï¼Œé¿å…çº¿ç¨‹æ± æ¸…ç†é˜¶æ®µäº§ç”Ÿå¤šä½™æ—¥å¿—
        os._exit(0)
    
    logging.info(f"\nè¯„æµ‹å®Œæˆï¼å…±ç”Ÿæˆ {len(output_files)} ä¸ªè¾“å‡ºæ–‡ä»¶")
    
    # å¤±è´¥æ‘˜è¦
    logging.info("\n" + "="*60)
    logging.info("å¤±è´¥æ‘˜è¦")
    logging.info("="*60)
    if not failures:
        logging.info("æ‰€æœ‰é—®é¢˜å‡å¤„ç†æˆåŠŸï¼Œæœªè®°å½•å¤±è´¥ã€‚")
    else:
        logging.info(f"å¤±è´¥æ€»æ•°: {len(failures)}")
        for fail in failures:
            logging.info(f"- question_id: {fail.get('question_id','')} | reason: {fail.get('reason','')}")
    # è¯¦ç»†æ—¥å¿—æ–‡ä»¶ä¹Ÿå†™å…¥å¤±è´¥æ‘˜è¦
    if DETAILED_LOG_FILE:
        with log_lock:
            try:
                DETAILED_LOG_FILE.write("=" * 80 + "\n")
                DETAILED_LOG_FILE.write("å¤±è´¥æ‘˜è¦\n")
                DETAILED_LOG_FILE.write("=" * 80 + "\n")
                if not failures:
                    DETAILED_LOG_FILE.write("æ‰€æœ‰é—®é¢˜å‡å¤„ç†æˆåŠŸï¼Œæœªè®°å½•å¤±è´¥ã€‚\n")
                else:
                    DETAILED_LOG_FILE.write(f"å¤±è´¥æ€»æ•°: {len(failures)}\n")
                    for fail in failures:
                        DETAILED_LOG_FILE.write(f"- question_id: {fail.get('question_id','')} | reason: {fail.get('reason','')}\n")
                DETAILED_LOG_FILE.write("\n")
                DETAILED_LOG_FILE.flush()
            except Exception as e:
                logging.warning(f"å†™å…¥å¤±è´¥æ‘˜è¦åˆ°è¯¦ç»†æ—¥å¿—å¤±è´¥: {e}")
    
    # æ‰“å°ç»Ÿè®¡æ‘˜è¦ï¼ˆä»è¾“å‡ºæ–‡ä»¶ä¸­è¯»å–ï¼‰
    logging.info("\n" + "="*60)
    logging.info("ç»Ÿè®¡æ‘˜è¦")
    logging.info("="*60)
    
    for model_name in enabled_models:
        for profile in profiles:
            key = (model_name, profile)
            output_file = output_files[key]
            
            try:
                if output_file.exists():
                    if output_format == "jsonl":
                        with open(output_file, 'r', encoding='utf-8') as f:
                            first_line = f.readline().strip()
                            if first_line:
                                stats_data = json.loads(first_line)
                                stats = stats_data.get("statistics", {})
                    else:
                        with open(output_file, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                            stats = data.get("statistics", {})
                    
                    total_stats = stats.get("total", {})
                    logging.info(f"\n{model_name} - {profile}:")
                    logging.info(f"  æ€»å‡†ç¡®ç‡: {total_stats.get('accuracy', 0):.2%} ({total_stats.get('correct_count', 0)}/{total_stats.get('total_count', 0)})")
            except Exception as e:
                logging.warning(f"è¯»å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥ ({model_name}, {profile}): {e}")
    
    
    # å…³é—­è¯¦ç»†æ—¥å¿—æ–‡ä»¶
    if DETAILED_LOG_FILE:
        with log_lock:
            try:
                DETAILED_LOG_FILE.write("=" * 80 + "\n")
                DETAILED_LOG_FILE.write(f"æ—¥å¿—ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                DETAILED_LOG_FILE.write("=" * 80 + "\n")
                DETAILED_LOG_FILE.close()
                DETAILED_LOG_FILE = None
            except Exception as e:
                logging.warning(f"å…³é—­è¯¦ç»†æ—¥å¿—æ–‡ä»¶å¤±è´¥: {e}")
                DETAILED_LOG_FILE = None


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='é‡‘èé¢†åŸŸå¤šç”¨æˆ·ç”»åƒè¯„æµ‹è„šæœ¬')
    parser.add_argument('--input_file', type=str, required=True, help='è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼ˆJSONæˆ–JSONLï¼‰')
    parser.add_argument('--output_file', type=str, default=None, 
                       help='è¾“å‡ºæ–‡ä»¶åï¼ˆå¯é€‰ï¼Œæ”¯æŒ .json æˆ– .jsonlï¼‰ã€‚æ–‡ä»¶å°†ä¿å­˜åœ¨ ./outputs/{profile}/{model_name}/ ç›®å½•ä¸‹')
    parser.add_argument('--log_dir', type=str, default='logs', help='æ—¥å¿—ç›®å½•')
    parser.add_argument('--log_level', type=str, default='INFO', help='æ—¥å¿—çº§åˆ«')
    parser.add_argument('--profiles', type=str, nargs='+', default=None, 
                       help='ç”¨æˆ·ç”»åƒåˆ—è¡¨ï¼ˆbeginner/retail/expert/expert_cotï¼‰ï¼Œé»˜è®¤å…¨éƒ¨')
    parser.add_argument('--resume', action='store_true', help='æ˜¯å¦å¯ç”¨æ–­ç‚¹ç»­è·‘ï¼ˆä»è¾“å‡ºæ–‡ä»¶ä¸­è¯»å–å·²å¤„ç†çš„é—®é¢˜ï¼‰')
    
    args = parser.parse_args()
    main(args)
