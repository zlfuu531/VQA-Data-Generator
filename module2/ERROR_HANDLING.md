# Module2 错误处理和断点续连功能说明

## 功能概述

Module2 现在支持自动检测模型生成错误，并在断点续连时自动重试错误的题目。

## 主要功能

### 1. 错误检测

系统会自动检测以下情况作为"模型生成错误"：

- **单轮题**：模型启用（`enabled=True`）但答案为空字符串
- **多轮题**：模型启用但答案不是字典格式或为空字典

如果三个模型中**任何一个**出现错误，该题目会被标记为错误题目。

### 2. 错误题目存储

错误的题目会被单独保存到 `error.json` 文件中，不会写入 `L1-L4.json` 文件。

**输出目录结构**：
```
output/module2/module2_result/
├── L1.json          # 难度1级别的正常题目
├── L2.json          # 难度2级别的正常题目
├── L3.json          # 难度3级别的正常题目
├── L4.json          # 难度4级别的正常题目
├── error.json       # 模型生成出错的题目
└── summary.json     # 统计信息（包含错误题目数量）
```

### 3. 断点续连和错误重试

当再次运行 Module2 时（`RE_EVALUATE=false`）：

1. **加载历史结果**：
   - 从 `L1-L4.json` 加载已成功处理的题目
   - 从 `error.json` 加载之前出错的题目

2. **处理流程**：
   - 跳过已在 `L1-L4.json` 中的题目（根据 `id` 判断）
   - 处理输入文件中的新题目
   - **自动重试** `error.json` 中的题目

3. **智能重试**：
   - 只重新调用**之前出错的模型**
   - 已有有效结果的模型会被跳过，避免重复调用
   - 例如：如果 model1 和 model2 正常，只有 model3 出错，则只重新调用 model3

4. **结果更新**：
   - 重试成功的题目会从 `error.json` 移至对应的 `L1-L4.json`
   - 仍然出错的题目会继续保留在 `error.json` 中

### 4. 错误信息记录

错误题目在 JSON 中会包含以下额外字段：

```json
{
  "id": "question_001",
  "question": "...",
  "answer": "...",
  "model_error": {
    "has_error": true,
    "error_models": ["model1", "model3"],
    "error_details": {
      "model1": "单轮题答案为空",
      "model3": "单轮题答案为空"
    }
  },
  ...
}
```

## 使用方法

### 正常运行（首次处理）

```bash
# 编辑 module2/main.sh，设置：
RE_EVALUATE=true  # 首次运行或完全重新评估

bash module2/main.sh
```

### 断点续连（重试错误）

```bash
# 编辑 module2/main.sh，设置：
RE_EVALUATE=false  # 增量模式，会自动重试 error.json 中的题目

bash module2/main.sh
```

## 工作流程示例

### 场景1：首次运行遇到错误

```
输入：100 道题
结果：
  - L1.json: 30 道（三个模型都正确）
  - L2.json: 25 道（两个模型正确）
  - L3.json: 20 道（一个模型正确）
  - L4.json: 10 道（全部模型错误）
  - error.json: 15 道（模型生成出错）
```

### 场景2：断点续连（自动重试错误）

```
设置 RE_EVALUATE=false，再次运行：

1. 加载历史结果：
   - 已处理：85 道（L1-L4）
   - 待重试：15 道（error.json）

2. 处理流程：
   - 跳过已处理的 85 道
   - 重试 error.json 中的 15 道（只调用出错的模型）

3. 假设重试后：
   - 10 道成功 → 移至 L1-L4
   - 5 道仍失败 → 继续保留在 error.json

最终结果：
  - L1-L4.json: 95 道
  - error.json: 5 道
```

### 场景3：添加新题目

```
在输入文件中添加 20 道新题，设置 RE_EVALUATE=false：

1. 加载历史结果：95 道
2. 处理新题目：20 道
3. 重试旧错误：5 道

最终结果：取决于新题目和重试的结果
```

## 统计信息

`summary.json` 会包含错误题目的统计：

```json
{
  "total_items": 95,      // 正常题目数量
  "error_items": 5,       // 错误题目数量
  "levels": {
    "L1": 30,
    "L2": 25,
    "L3": 20,
    "L4": 20
  },
  ...
}
```

## 注意事项

1. **错误判断标准**：
   - 只检查答案是否为空，不检查答案的正确性
   - API 调用失败、超时等都会导致答案为空，从而被标记为错误

2. **重试策略**：
   - 每次运行只会重试一次 `error.json` 中的题目
   - 如果仍然失败，需要再次运行才会再次重试
   - 建议检查模型配置、API密钥、网络连接等问题

3. **完全重新评估**：
   - 设置 `RE_EVALUATE=true` 会生成新的版本目录（如 `module2_result_v2`）
   - 不会读取旧的 `error.json`，所有题目都会重新处理

4. **并发处理**：
   - 错误重试时也会并发处理，遵循 `WORKERS` 参数设置
   - 对于同一道题目，不同模型会并行调用（跳过已有结果的模型）

## 技术实现

### 核心修改

1. **`model_evaluation.py`**：
   - 新增 `_check_model_errors()` 方法：检测模型错误
   - 修改 `evaluate_item()`：支持错误重试模式
   - 修改 `step1_call_models()`：支持跳过已有结果的模型
   - 修改 `_load_existing_results()`：加载 `error.json`
   - 修改 `_save_by_level_and_summary()`：分离保存错误题目
   - 修改 `batch_evaluate()`：添加错误重试流程

2. **`answer_comparison.py`**：
   - 修改 `compare_three_models()`：检查已有结果，跳过已成功的模型

### 数据流程

```
输入题目
    ↓
调用模型（跳过已有结果）
    ↓
检查错误？
    ↙        ↘
  是          否
    ↓          ↓
error.json  评判+分级
            ↓
          L1-L4.json
```

## 日志输出

运行时会输出以下信息：

```
🔁 检测到已有输出目录，历史样本数: 85
🔄 检测到错误文件，错误样本数: 15
📊 总数: 100 | 已处理: 85 | 新增待处理: 0 | 错误重试: 15

🔄 重试错误样本...
    模型1 已有结果，跳过调用
    模型2 已有结果，跳过调用
    并行调用 1 个模型...
    ⚠️  模型生成错误 (item question_001): ['model3']

⚠️  已输出错误样本 5 条 -> .../error.json
💾 已输出 L1 级样本 35 条 -> .../L1.json
...
```

## 常见问题

### Q: 为什么某些题目一直在 error.json 中？

A: 可能的原因：
- API 密钥错误或过期
- 模型服务不可用
- 网络连接问题
- 题目本身导致模型无法生成答案

建议：
1. 检查 `module2_logs/` 中的日志文件
2. 设置 `DEBUG_MODE=true` 查看详细错误信息
3. 手动测试单个模型的 API 调用

### Q: 如何强制重新处理所有题目（包括错误的）？

A: 设置 `RE_EVALUATE=true`，这会生成一个新的版本目录，不会读取旧的结果。

### Q: error.json 中的题目会影响统计吗？

A: 不会。`summary.json` 中的统计只包含正常处理的题目（L1-L4），错误题目单独统计在 `error_items` 字段中。

### Q: 可以手动编辑 error.json 吗？

A: 可以，但需要保持 JSON 格式正确。可以：
- 删除某些题目（不再重试）
- 修改题目内容
- 清除 `model_error` 字段（系统会重新评判）

## 最佳实践

1. **首次运行**：
   - 使用 `RE_EVALUATE=true`
   - 检查 `error.json`，确认是否有错误

2. **定期重试**：
   - 使用 `RE_EVALUATE=false`
   - 多次运行以自动重试错误题目

3. **监控错误率**：
   - 查看 `summary.json` 中的 `error_items` 数量
   - 如果错误率过高，检查模型配置

4. **日志分析**：
   - 查看 `module2_logs/` 中的详细日志
   - 使用 `DEBUG_MODE=true` 获取更多信息

