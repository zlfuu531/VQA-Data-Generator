"""
æµ‹è¯•é”™è¯¯å¤„ç†åŠŸèƒ½çš„ç®€å•è„šæœ¬
å¯ä»¥åˆ›å»ºä¸€äº›æ¨¡æ‹Ÿæ•°æ®æ¥æµ‹è¯•é”™è¯¯æ£€æµ‹å’Œé‡è¯•é€»è¾‘
"""
import os
import sys
import json

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from module2.model_evaluation import Module2ModelEvaluation

def test_check_model_errors():
    """æµ‹è¯•é”™è¯¯æ£€æµ‹åŠŸèƒ½"""
    print("=" * 60)
    print("æµ‹è¯•1: é”™è¯¯æ£€æµ‹åŠŸèƒ½")
    print("=" * 60)
    
    evaluator = Module2ModelEvaluation(debug_mode=True)
    
    # æµ‹è¯•ç”¨ä¾‹1ï¼šæ­£å¸¸æƒ…å†µï¼ˆæ— é”™è¯¯ï¼‰
    item1 = {
        "id": "test_001",
        "question": "æµ‹è¯•é—®é¢˜",
        "answer": "æµ‹è¯•ç­”æ¡ˆ",
        "model1": {
            "enabled": True,
            "answer": "æ¨¡å‹1çš„ç­”æ¡ˆ",
            "process": "æ¨¡å‹1çš„æ¨ç†è¿‡ç¨‹"
        },
        "model2": {
            "enabled": True,
            "answer": "æ¨¡å‹2çš„ç­”æ¡ˆ",
            "process": "æ¨¡å‹2çš„æ¨ç†è¿‡ç¨‹"
        },
        "model3": {
            "enabled": True,
            "answer": "æ¨¡å‹3çš„ç­”æ¡ˆ",
            "process": "æ¨¡å‹3çš„æ¨ç†è¿‡ç¨‹"
        }
    }
    
    error_info = evaluator._check_model_errors(item1)
    print("\næµ‹è¯•ç”¨ä¾‹1ï¼šæ‰€æœ‰æ¨¡å‹éƒ½æ­£å¸¸")
    print(f"  has_error: {error_info['has_error']}")
    print(f"  error_models: {error_info['error_models']}")
    assert error_info['has_error'] == False, "åº”è¯¥æ²¡æœ‰é”™è¯¯"
    print("  âœ… é€šè¿‡")
    
    # æµ‹è¯•ç”¨ä¾‹2ï¼šmodel1 å‡ºé”™ï¼ˆå•è½®é¢˜ç­”æ¡ˆä¸ºç©ºï¼‰
    item2 = {
        "id": "test_002",
        "question": "æµ‹è¯•é—®é¢˜",
        "answer": "æµ‹è¯•ç­”æ¡ˆ",
        "model1": {
            "enabled": True,
            "answer": "",  # ç©ºç­”æ¡ˆ
            "process": "æ¨¡å‹1çš„æ¨ç†è¿‡ç¨‹"
        },
        "model2": {
            "enabled": True,
            "answer": "æ¨¡å‹2çš„ç­”æ¡ˆ",
            "process": "æ¨¡å‹2çš„æ¨ç†è¿‡ç¨‹"
        },
        "model3": {
            "enabled": True,
            "answer": "æ¨¡å‹3çš„ç­”æ¡ˆ",
            "process": "æ¨¡å‹3çš„æ¨ç†è¿‡ç¨‹"
        }
    }
    
    error_info = evaluator._check_model_errors(item2)
    print("\næµ‹è¯•ç”¨ä¾‹2ï¼šmodel1 å‡ºé”™ï¼ˆç­”æ¡ˆä¸ºç©ºï¼‰")
    print(f"  has_error: {error_info['has_error']}")
    print(f"  error_models: {error_info['error_models']}")
    print(f"  error_details: {error_info['error_details']}")
    assert error_info['has_error'] == True, "åº”è¯¥æœ‰é”™è¯¯"
    assert "model1" in error_info['error_models'], "model1 åº”è¯¥åœ¨é”™è¯¯åˆ—è¡¨ä¸­"
    print("  âœ… é€šè¿‡")
    
    # æµ‹è¯•ç”¨ä¾‹3ï¼šå¤šä¸ªæ¨¡å‹å‡ºé”™
    item3 = {
        "id": "test_003",
        "question": "æµ‹è¯•é—®é¢˜",
        "answer": "æµ‹è¯•ç­”æ¡ˆ",
        "model1": {
            "enabled": True,
            "answer": "",  # ç©ºç­”æ¡ˆ
            "process": ""
        },
        "model2": {
            "enabled": True,
            "answer": "æ¨¡å‹2çš„ç­”æ¡ˆ",
            "process": "æ¨¡å‹2çš„æ¨ç†è¿‡ç¨‹"
        },
        "model3": {
            "enabled": True,
            "answer": "  ",  # åªæœ‰ç©ºæ ¼
            "process": ""
        }
    }
    
    error_info = evaluator._check_model_errors(item3)
    print("\næµ‹è¯•ç”¨ä¾‹3ï¼šmodel1 å’Œ model3 å‡ºé”™")
    print(f"  has_error: {error_info['has_error']}")
    print(f"  error_models: {error_info['error_models']}")
    print(f"  error_details: {error_info['error_details']}")
    assert error_info['has_error'] == True, "åº”è¯¥æœ‰é”™è¯¯"
    assert "model1" in error_info['error_models'], "model1 åº”è¯¥åœ¨é”™è¯¯åˆ—è¡¨ä¸­"
    assert "model3" in error_info['error_models'], "model3 åº”è¯¥åœ¨é”™è¯¯åˆ—è¡¨ä¸­"
    print("  âœ… é€šè¿‡")
    
    # æµ‹è¯•ç”¨ä¾‹4ï¼šå¤šè½®é¢˜å‡ºé”™
    item4 = {
        "id": "test_004",
        "question": {"round1": "é—®é¢˜1", "round2": "é—®é¢˜2"},  # å¤šè½®é¢˜
        "answer": {"round1": "ç­”æ¡ˆ1", "round2": "ç­”æ¡ˆ2"},
        "model1": {
            "enabled": True,
            "answer": {"round1": "ç­”æ¡ˆ1", "round2": "ç­”æ¡ˆ2"},  # æ­£å¸¸
            "process": {"round1": "æ¨ç†1", "round2": "æ¨ç†2"}
        },
        "model2": {
            "enabled": True,
            "answer": {},  # ç©ºå­—å…¸
            "process": {}
        },
        "model3": {
            "enabled": True,
            "answer": {"round1": "ç­”æ¡ˆ1"},  # ç¼ºå°‘ round2
            "process": {"round1": "æ¨ç†1"}
        }
    }
    
    error_info = evaluator._check_model_errors(item4)
    print("\næµ‹è¯•ç”¨ä¾‹4ï¼šå¤šè½®é¢˜ï¼Œmodel2 å‡ºé”™ï¼ˆç­”æ¡ˆä¸ºç©ºå­—å…¸ï¼‰")
    print(f"  has_error: {error_info['has_error']}")
    print(f"  error_models: {error_info['error_models']}")
    print(f"  error_details: {error_info['error_details']}")
    assert error_info['has_error'] == True, "åº”è¯¥æœ‰é”™è¯¯"
    assert "model2" in error_info['error_models'], "model2 åº”è¯¥åœ¨é”™è¯¯åˆ—è¡¨ä¸­"
    # æ³¨æ„ï¼šmodel3 è™½ç„¶ç¼ºå°‘ round2ï¼Œä½†ç­”æ¡ˆä¸ä¸ºç©ºï¼Œæ‰€ä»¥ä¸ç®—é”™è¯¯ï¼ˆè¿™æ˜¯ä¸šåŠ¡é€»è¾‘çš„é€‰æ‹©ï¼‰
    print("  âœ… é€šè¿‡")
    
    # æµ‹è¯•ç”¨ä¾‹5ï¼šç¦ç”¨çš„æ¨¡å‹ä¸æ£€æŸ¥
    item5 = {
        "id": "test_005",
        "question": "æµ‹è¯•é—®é¢˜",
        "answer": "æµ‹è¯•ç­”æ¡ˆ",
        "model1": {
            "enabled": False,  # ç¦ç”¨
            "answer": "",  # å³ä½¿ä¸ºç©ºä¹Ÿä¸ç®—é”™è¯¯
            "process": ""
        },
        "model2": {
            "enabled": True,
            "answer": "æ¨¡å‹2çš„ç­”æ¡ˆ",
            "process": "æ¨¡å‹2çš„æ¨ç†è¿‡ç¨‹"
        },
        "model3": {
            "enabled": True,
            "answer": "æ¨¡å‹3çš„ç­”æ¡ˆ",
            "process": "æ¨¡å‹3çš„æ¨ç†è¿‡ç¨‹"
        }
    }
    
    error_info = evaluator._check_model_errors(item5)
    print("\næµ‹è¯•ç”¨ä¾‹5ï¼šmodel1 ç¦ç”¨ï¼ˆç­”æ¡ˆä¸ºç©ºä½†ä¸ç®—é”™è¯¯ï¼‰")
    print(f"  has_error: {error_info['has_error']}")
    print(f"  error_models: {error_info['error_models']}")
    assert error_info['has_error'] == False, "ä¸åº”è¯¥æœ‰é”™è¯¯ï¼ˆmodel1 è¢«ç¦ç”¨ï¼‰"
    print("  âœ… é€šè¿‡")
    
    print("\n" + "=" * 60)
    print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
    print("=" * 60)


def test_save_error_separation():
    """æµ‹è¯•ä¿å­˜æ—¶é”™è¯¯åˆ†ç¦»åŠŸèƒ½"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•2: ä¿å­˜æ—¶é”™è¯¯åˆ†ç¦»")
    print("=" * 60)
    
    # åˆ›å»ºä¸´æ—¶æµ‹è¯•ç›®å½•
    import tempfile
    import shutil
    
    test_dir = tempfile.mkdtemp(prefix="module2_test_")
    print(f"\nä¸´æ—¶æµ‹è¯•ç›®å½•: {test_dir}")
    
    try:
        evaluator = Module2ModelEvaluation(output_dir=test_dir, debug_mode=True)
        
        # æ¨¡æ‹Ÿç»“æœï¼šåŒ…å«æ­£å¸¸å’Œé”™è¯¯çš„é¢˜ç›®
        results = [
            # L1: ä¸‰ä¸ªæ¨¡å‹éƒ½å¯¹
            {
                "id": "q001",
                "question": "é—®é¢˜1",
                "answer": "ç­”æ¡ˆ1",
                "question_type": "å•é€‰é¢˜",
                "image_type": "å›¾è¡¨",
                "model1": {"enabled": True, "answer": "A", "match_gt": True},
                "model2": {"enabled": True, "answer": "A", "match_gt": True},
                "model3": {"enabled": True, "answer": "A", "match_gt": True},
                "classification": {"level": "L1", "category": "ä¸‰ä¸ªæ¨¡å‹éƒ½å’ŒGTç›¸åŒ", "agreement_count": 3}
            },
            # L2: ä¸¤ä¸ªæ¨¡å‹å¯¹
            {
                "id": "q002",
                "question": "é—®é¢˜2",
                "answer": "ç­”æ¡ˆ2",
                "question_type": "å•é€‰é¢˜",
                "image_type": "å›¾è¡¨",
                "model1": {"enabled": True, "answer": "B", "match_gt": True},
                "model2": {"enabled": True, "answer": "B", "match_gt": True},
                "model3": {"enabled": True, "answer": "C", "match_gt": False},
                "classification": {"level": "L2", "category": "ä¸¤ä¸ªæ¨¡å‹å’ŒGTç›¸åŒ", "agreement_count": 2}
            },
            # é”™è¯¯1: model1 å‡ºé”™
            {
                "id": "q003",
                "question": "é—®é¢˜3",
                "answer": "ç­”æ¡ˆ3",
                "question_type": "å•é€‰é¢˜",
                "image_type": "å›¾è¡¨",
                "model1": {"enabled": True, "answer": "", "match_gt": False},
                "model2": {"enabled": True, "answer": "D", "match_gt": False},
                "model3": {"enabled": True, "answer": "D", "match_gt": False},
                "model_error": {
                    "has_error": True,
                    "error_models": ["model1"],
                    "error_details": {"model1": "å•è½®é¢˜ç­”æ¡ˆä¸ºç©º"}
                }
            },
            # L3: ä¸€ä¸ªæ¨¡å‹å¯¹
            {
                "id": "q004",
                "question": "é—®é¢˜4",
                "answer": "ç­”æ¡ˆ4",
                "question_type": "å¤šé€‰é¢˜",
                "image_type": "ç…§ç‰‡",
                "model1": {"enabled": True, "answer": "AB", "match_gt": True},
                "model2": {"enabled": True, "answer": "AC", "match_gt": False},
                "model3": {"enabled": True, "answer": "BC", "match_gt": False},
                "classification": {"level": "L3", "category": "ä¸€ä¸ªæ¨¡å‹å’ŒGTç›¸åŒ", "agreement_count": 1}
            },
            # é”™è¯¯2: model2 å’Œ model3 å‡ºé”™
            {
                "id": "q005",
                "question": "é—®é¢˜5",
                "answer": "ç­”æ¡ˆ5",
                "question_type": "åˆ¤æ–­é¢˜",
                "image_type": "å›¾è¡¨",
                "model1": {"enabled": True, "answer": "æ­£ç¡®", "match_gt": False},
                "model2": {"enabled": True, "answer": "", "match_gt": False},
                "model3": {"enabled": True, "answer": "  ", "match_gt": False},
                "model_error": {
                    "has_error": True,
                    "error_models": ["model2", "model3"],
                    "error_details": {
                        "model2": "å•è½®é¢˜ç­”æ¡ˆä¸ºç©º",
                        "model3": "å•è½®é¢˜ç­”æ¡ˆä¸ºç©º"
                    }
                }
            }
        ]
        
        # ä¿å­˜ç»“æœ
        output_file = os.path.join(test_dir, "test_result.json")
        evaluator._save_by_level_and_summary(results, output_file)
        
        # æ£€æŸ¥è¾“å‡º
        output_dir = os.path.join(test_dir, "test_result")
        
        # æ£€æŸ¥ L1.json
        l1_path = os.path.join(output_dir, "L1.json")
        assert os.path.exists(l1_path), "L1.json åº”è¯¥å­˜åœ¨"
        with open(l1_path) as f:
            l1_data = json.load(f)
        print(f"\nL1.json: {len(l1_data)} æ¡")
        assert len(l1_data) == 1, "L1 åº”è¯¥æœ‰ 1 æ¡"
        assert l1_data[0]["id"] == "q001", "L1 åº”è¯¥æ˜¯ q001"
        print("  âœ… L1.json æ­£ç¡®")
        
        # æ£€æŸ¥ L2.json
        l2_path = os.path.join(output_dir, "L2.json")
        assert os.path.exists(l2_path), "L2.json åº”è¯¥å­˜åœ¨"
        with open(l2_path) as f:
            l2_data = json.load(f)
        print(f"L2.json: {len(l2_data)} æ¡")
        assert len(l2_data) == 1, "L2 åº”è¯¥æœ‰ 1 æ¡"
        print("  âœ… L2.json æ­£ç¡®")
        
        # æ£€æŸ¥ L3.json
        l3_path = os.path.join(output_dir, "L3.json")
        assert os.path.exists(l3_path), "L3.json åº”è¯¥å­˜åœ¨"
        with open(l3_path) as f:
            l3_data = json.load(f)
        print(f"L3.json: {len(l3_data)} æ¡")
        assert len(l3_data) == 1, "L3 åº”è¯¥æœ‰ 1 æ¡"
        print("  âœ… L3.json æ­£ç¡®")
        
        # æ£€æŸ¥ error.json
        error_path = os.path.join(output_dir, "error.json")
        assert os.path.exists(error_path), "error.json åº”è¯¥å­˜åœ¨"
        with open(error_path) as f:
            error_data = json.load(f)
        print(f"error.json: {len(error_data)} æ¡")
        assert len(error_data) == 2, "error.json åº”è¯¥æœ‰ 2 æ¡"
        assert error_data[0]["id"] == "q003", "ç¬¬ä¸€ä¸ªé”™è¯¯åº”è¯¥æ˜¯ q003"
        assert error_data[1]["id"] == "q005", "ç¬¬äºŒä¸ªé”™è¯¯åº”è¯¥æ˜¯ q005"
        print("  âœ… error.json æ­£ç¡®")
        
        # æ£€æŸ¥ summary.json
        summary_path = os.path.join(output_dir, "summary.json")
        assert os.path.exists(summary_path), "summary.json åº”è¯¥å­˜åœ¨"
        with open(summary_path) as f:
            summary = json.load(f)
        print(f"\nsummary.json:")
        print(f"  total_items: {summary['total_items']}")
        print(f"  error_items: {summary['error_items']}")
        assert summary["total_items"] == 3, "æ­£å¸¸é¢˜ç›®åº”è¯¥æœ‰ 3 æ¡"
        assert summary["error_items"] == 2, "é”™è¯¯é¢˜ç›®åº”è¯¥æœ‰ 2 æ¡"
        print("  âœ… summary.json æ­£ç¡®")
        
        print("\n" + "=" * 60)
        print("âœ… ä¿å­˜åŠŸèƒ½æµ‹è¯•é€šè¿‡ï¼")
        print("=" * 60)
        
    finally:
        # æ¸…ç†æµ‹è¯•ç›®å½•
        if os.path.exists(test_dir):
            shutil.rmtree(test_dir)
            print(f"\nå·²æ¸…ç†æµ‹è¯•ç›®å½•: {test_dir}")


if __name__ == "__main__":
    try:
        # è¿è¡Œæµ‹è¯•
        test_check_model_errors()
        test_save_error_separation()
        
        print("\n" + "=" * 60)
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼é”™è¯¯å¤„ç†åŠŸèƒ½æ­£å¸¸å·¥ä½œã€‚")
        print("=" * 60)
        
    except AssertionError as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

