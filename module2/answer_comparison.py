"""
ç­”æ¡ˆå¯¹æ¯”æ¨¡å—
å¯¹æ¯”ä¸‰ä¸ªæ¨¡å‹çš„ç­”æ¡ˆ
ä½¿ç”¨è§„èŒƒçš„OpenAIæ ¼å¼è°ƒç”¨æ¨¡å‹ï¼Œæ”¯æŒè‡ªå®šä¹‰æ¨¡å‹åç§°
æ”¯æŒå¹¶è¡Œè°ƒç”¨ä¸‰ä¸ªæ¨¡å‹ä»¥æé«˜é€Ÿåº¦
"""
import os
import sys
import time
import json
import re
from typing import List, Dict, Tuple, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from module2.config import MODEL_CONFIG
from utils import count_agreement
from models.model1 import call_model1_api
from models.model2 import call_model2_api
from models.model3 import call_model3_api
from module2.logger import log_model_response, log_question_start


class AnswerComparison:
    """ç­”æ¡ˆå¯¹æ¯”å™¨"""
    
    def __init__(self, debug_mode: bool = False):
        self.model1_config = MODEL_CONFIG["model1"]
        self.model2_config = MODEL_CONFIG["model2"]
        self.model3_config = MODEL_CONFIG["model3"]
        self.model1_enabled = self.model1_config.get("enabled", True)
        self.model2_enabled = self.model2_config.get("enabled", True)
        self.model3_enabled = self.model3_config.get("enabled", True)
        self.question_count = 0  # ç”¨äºè·Ÿè¸ªé—®é¢˜æ•°é‡ï¼Œåˆ¤æ–­æ˜¯å¦æ˜¯ç¬¬ä¸€ä¸ªé—®é¢˜
        self.debug_mode = debug_mode
        self.current_question_id = "unknown"  # å½“å‰æ­£åœ¨å¤„ç†çš„é—®é¢˜ID
        
        # ä½¿ç”¨MODEL_CONFIGä¸­çš„è‡ªå®šä¹‰æ¨¡å‹åç§°ï¼ˆapi_config_nameï¼‰
        # å¿…é¡»ä»MODEL_CONFIGä¸­è·å–ï¼Œä¸èƒ½æœ‰é»˜è®¤å€¼
        self.model1_api_config_name = self.model1_config.get("name")
        self.model2_api_config_name = self.model2_config.get("name")
        self.model3_api_config_name = self.model3_config.get("name")
        
        if not self.model1_api_config_name:
            raise ValueError("MODEL_CONFIG['model1']['name'] å¿…é¡»é…ç½®ï¼ŒæŒ‡å‘ API_CONFIG ä¸­çš„æŸä¸ª key")
        if not self.model2_api_config_name:
            raise ValueError("MODEL_CONFIG['model2']['name'] å¿…é¡»é…ç½®ï¼ŒæŒ‡å‘ API_CONFIG ä¸­çš„æŸä¸ª key")
        if not self.model3_api_config_name:
            raise ValueError("MODEL_CONFIG['model3']['name'] å¿…é¡»é…ç½®ï¼ŒæŒ‡å‘ API_CONFIG ä¸­çš„æŸä¸ª key")
    
    def get_model_answer(self, model_num: int, api_config_name: str, enabled: bool, 
                         question: str, image_path: str = "") -> Tuple[str, str, float, Optional[dict], str]:
        """
        è·å–æ¨¡å‹ç­”æ¡ˆï¼ˆç›´æ¥è°ƒç”¨æ¨¡å‹APIå‡½æ•°ï¼‰
        
        âš ï¸ æç¤ºè¯æ‹¼æ¥è¯´æ˜ï¼š
        - question å‚æ•°ï¼š
          * å•è½®é¢˜ï¼šæ¥è‡ª model_evaluation.py::_build_model_question() æ„å»ºçš„å®Œæ•´é—®é¢˜æ–‡æœ¬
          * å¤šè½®é¢˜ï¼šæ¥è‡ª get_model_answer_multi_round() æ„é€ çš„ round_question
        - åœ¨ call_model*_api() ä¸­ï¼Œä½¿ç”¨ PROMPT_TEMPLATE.format(question=question) æ‹¼æ¥
        - æœ€ç»ˆæç¤ºè¯ = PROMPT_TEMPLATE + question
        - è¯¦è§ module2/PROMPT_FLOW.md
        
        Args:
            model_num: æ¨¡å‹ç¼–å·ï¼ˆ1, 2, æˆ– 3ï¼‰
            api_config_name: APIé…ç½®åç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰
            enabled: æ˜¯å¦å¯ç”¨
            question: é—®é¢˜æ–‡æœ¬ï¼ˆå·²åŒ…å«æ ¼å¼è¦æ±‚ï¼‰
            image_path: å›¾ç‰‡è·¯å¾„
        
        Returns:
            (answer, process, response_time, raw_response_json, final_prompt): 
            - answer: ä» \boxed{} ä¸­æå–çš„ç­”æ¡ˆ
            - process: æ¨ç†è¿‡ç¨‹ï¼ˆå»é™¤ \boxed{} åçš„æ–‡æœ¬ï¼‰
            - response_time: å“åº”æ—¶é—´ï¼ˆç§’ï¼‰
            - raw_response_json: åŸå§‹APIå“åº”ï¼ˆå­—å…¸æ ¼å¼ï¼‰
            - final_prompt: æœ€ç»ˆæäº¤ç»™æ¨¡å‹çš„å®Œæ•´æç¤ºè¯ï¼ˆç”¨äºæ—¥å¿—è®°å½•ï¼‰
        """
        if not enabled:
            print(f"    æ¨¡å‹{model_num} ({api_config_name}) å·²ç¦ç”¨ï¼Œè·³è¿‡")
            return "", "", 0.0, None, ""
        
        # éªŒè¯è¾“å…¥
        if not question or not question.strip():
            print(f"      âš ï¸ è­¦å‘Šï¼šæ¨¡å‹{model_num} çš„é—®é¢˜ä¸ºç©ºï¼Œè·³è¿‡è°ƒç”¨")
            return "", "", 0.0, None, ""
        
        try:
            print(f"      è°ƒç”¨æ¨¡å‹{model_num} API: {api_config_name}, é—®é¢˜é•¿åº¦: {len(question)}, å›¾ç‰‡: {image_path if image_path else 'æ— '}")
            
            # ç›´æ¥è°ƒç”¨å¯¹åº”çš„æ¨¡å‹APIå‡½æ•°
            if model_num == 1:
                result = call_model1_api(question, image_path if image_path else None)
            elif model_num == 2:
                result = call_model2_api(question, image_path if image_path else None)
            elif model_num == 3:
                result = call_model3_api(question, image_path if image_path else None)
            else:
                raise ValueError(f"æœªçŸ¥çš„æ¨¡å‹ç¼–å·: {model_num}ï¼Œåªæ”¯æŒ 1, 2, 3")
            
            # resultæ ¼å¼: [process, answer, response_time, raw_response_json, final_prompt]
            # éœ€è¦éªŒè¯è¿”å›æ ¼å¼
            if not isinstance(result, (list, tuple)) or len(result) < 2:
                print(f"      âš ï¸ è­¦å‘Šï¼šæ¨¡å‹{model_num} è¿”å›æ ¼å¼å¼‚å¸¸: {result}")
                return "", "", 0.0, None, question
            
            # æ ¹æ®å®é™…è¿”å›æ ¼å¼è§£æï¼ˆæ ¼å¼ä¸º [process, answer, response_time, raw_response_json, final_prompt]ï¼‰
            process = str(result[0]) if len(result) > 0 and result[0] else ""
            answer = str(result[1]) if len(result) > 1 and result[1] else ""
            response_time = float(result[2]) if len(result) > 2 and result[2] else 0.0
            raw_response_json = result[3] if len(result) > 3 else None
            final_prompt = result[4] if len(result) > 4 else question  # å®Œæ•´çš„æœ€ç»ˆæç¤ºè¯ï¼Œç”¨äºæ—¥å¿—
            
            # éªŒè¯ç»“æœ
            if not answer:
                print(f"      âš ï¸ è­¦å‘Šï¼šæ¨¡å‹{model_num} è¿”å›çš„ç­”æ¡ˆä¸ºç©º")
            
            print(f"      æ¨¡å‹{model_num} APIè°ƒç”¨å®Œæˆï¼Œè€—æ—¶: {response_time:.2f}ç§’")
            print(f"      è§£æç»“æœ: answeré•¿åº¦={len(answer)}, processé•¿åº¦={len(process)}")
            
            return answer, process, response_time, raw_response_json, final_prompt
        except Exception as e:
            import traceback
            print(f"      âŒ è·å–æ¨¡å‹{model_num} ({api_config_name}) ç­”æ¡ˆæ—¶å‡ºé”™: {e}")
            if self.debug_mode:
                print(f"      é”™è¯¯è¯¦æƒ…:")
                traceback.print_exc()
            return "", "", 0.0, None, question
    
    def get_model_answer_multi_round(
        self,
        model_num: int,
        api_config_name: str,
        enabled: bool,
        question_rounds: Dict[str, str],
        image_path: str = "",
        question_id: str = "unknown",
        question_num: int = 0,
        format_requirements: str = "",  # å·²åºŸå¼ƒï¼Œä¿ç•™ä»¥å…¼å®¹æ¥å£
    ) -> Tuple[Dict[str, str], Dict[str, str], float, Optional[dict], str]:
        """
        é’ˆå¯¹å¤šè½®å¯¹è¯é¢˜å‹ï¼ŒæŒ‰è½®ä¾æ¬¡æé—®ï¼ŒåŒä¸€ä¸ªæ¨¡å‹å¤šæ¬¡è°ƒç”¨ï¼š
        - question_rounds: {"round1": "......", "round2": "...", ...}
        - format_requirements: æ ¼å¼è¦æ±‚æ–‡æœ¬ï¼ˆä» _build_model_question ä¸­æå–çš„å¤šè½®é¢˜æ ¼å¼è¦æ±‚ï¼‰
        
        âš ï¸ æç¤ºè¯æ‹¼æ¥æµç¨‹ï¼š
        1. ç¬¬ä¸€è½®ï¼šround_question = "round1ï¼š{é—®é¢˜æ–‡æœ¬}"
        2. åç»­è½®ï¼šround_question = "å†å²å¯¹è¯\n\nç°åœ¨æ˜¯æ–°çš„è½®æ¬¡ roundXï¼Œè¯·åªå›ç­”æœ¬è½®é—®é¢˜ï¼š{é—®é¢˜æ–‡æœ¬}"
        3. round_question ä¼ ç»™ get_model_answer -> call_model*_api
        4. call_model*_api ä½¿ç”¨ PROMPT_TEMPLATE.format(question=round_question) æ‹¼æ¥
        5. æœ€ç»ˆæç¤ºè¯ = PROMPT_TEMPLATE + round_questionï¼ˆæ ¼å¼è¦æ±‚å·²åœ¨ PROMPT_TEMPLATE ä¸­ç»Ÿä¸€è¯´æ˜ï¼‰
        
        è¿”å›å€¼ï¼š
        - final_answer: å­—å…¸æ ¼å¼ï¼Œä¾‹å¦‚ {"round1": "ç­”æ¡ˆ1", "round2": "ç­”æ¡ˆ2"}ï¼Œæ¯ä¸ª round éƒ½æ˜¯ç‹¬ç«‹çš„
        - final_process: å­—å…¸æ ¼å¼ï¼Œä¾‹å¦‚ {"round1": "æ¨ç†è¿‡ç¨‹1", "round2": "æ¨ç†è¿‡ç¨‹2"}ï¼Œæ¯ä¸ª round éƒ½æ˜¯ç‹¬ç«‹çš„
        - total_time: æ‰€æœ‰è½®æ¬¡è€—æ—¶ä¹‹å’Œ
        - last_raw_json: æœ€åä¸€è½®çš„åŸå§‹å“åº”JSON
        - last_final_prompt: æœ€åä¸€è½®çš„å®Œæ•´æç¤ºè¯ï¼ˆç”¨äºæ—¥å¿—ï¼‰
        """
        if not enabled:
            print(f"    æ¨¡å‹{model_num} ({api_config_name}) å·²ç¦ç”¨ï¼ˆå¤šè½®ï¼‰ï¼Œè·³è¿‡")
            return "", "", 0.0, None, ""

        answers: Dict[str, str] = {}
        processes: Dict[str, str] = {}
        total_time = 0.0
        last_raw_json: Optional[dict] = None
        last_final_prompt: str = ""  # æœ€åä¸€è½®çš„å®Œæ•´æç¤ºè¯
        # å¸¦ä¸Šä¸‹æ–‡çš„å¯¹è¯å†å²ï¼ˆæŒ‰æ¨¡å‹è‡ªå·±çš„å›ç­”ç´¯ç§¯ï¼‰
        history_segments = []
        
        def _round_sort_key(k: str) -> Tuple[int, str]:
            """
            è½®æ¬¡æ’åºå‡½æ•°ï¼š
            - ä¼˜å…ˆæå–å…¶ä¸­çš„æ•°å­—éƒ¨åˆ†ï¼ˆå¦‚ round10 -> 10ï¼‰ï¼ŒæŒ‰æ•°å­—å‡åº
            - è‹¥æå–ä¸åˆ°æ•°å­—ï¼Œåˆ™æŒ‰åŸå­—ç¬¦ä¸²æ’åº
            """
            m = re.search(r"(\d+)", str(k))
            if m:
                return int(m.group(1)), str(k)
            return 0, str(k)
        
        # æŒ‰ç…§â€œæ•°å­—ä¼˜å…ˆâ€çš„é¡ºåºéå†å„è½®
        for round_key in sorted(question_rounds.keys(), key=_round_sort_key):
            q_text = question_rounds.get(round_key, "")
            if not q_text:
                continue

            # æ„é€ å¸¦ä¸Šä¸‹æ–‡çš„é—®é¢˜ï¼š
            # - å‰å‡ è½®çš„é—®ç­”ä½œä¸º"å¯¹è¯å†å²"
            # - å½“å‰è½®çš„é—®é¢˜å•ç‹¬æ ‡å‡º
            # æ³¨æ„ï¼šæ ¼å¼è¦æ±‚å·²åœ¨ PROMPT_TEMPLATE ä¸­ç»Ÿä¸€è¯´æ˜ï¼Œè¿™é‡Œä¸éœ€è¦é¢å¤–æ·»åŠ 
            if history_segments:
                history_text = "\n".join(history_segments)
                round_question = (
                    f"ä¸‹é¢æ˜¯æˆ‘ä»¬ä¹‹å‰çš„å¯¹è¯å†å²ï¼ˆä¾›ä½ å‚è€ƒï¼Œä¸è¦é‡å¤å›ç­”ï¼‰ï¼š\n"
                    f"{history_text}\n\n"
                    f"ç°åœ¨æ˜¯æ–°çš„è½®æ¬¡ {round_key}ï¼Œè¯·åªå›ç­”æœ¬è½®é—®é¢˜ï¼š\n{q_text}"
                )
            else:
                # ç¬¬ä¸€è½®ï¼šç›´æ¥ä½¿ç”¨é—®é¢˜æ–‡æœ¬
                round_question = f"{round_key}ï¼š{q_text}"

            print(f"    [å¤šè½®] æ¨¡å‹{model_num} ({api_config_name}) -> {round_key}")

            ans, proc, rt, raw, final_prompt = self.get_model_answer(
                model_num=model_num,
                api_config_name=api_config_name,
                enabled=enabled,
                question=round_question,
                image_path=image_path,
            )

            answers[round_key] = ans
            processes[round_key] = proc
            total_time += rt
            last_raw_json = raw
            last_final_prompt = final_prompt  # ä¿å­˜æœ€åä¸€è½®çš„å®Œæ•´æç¤ºè¯
            
            # è®°å½•æ¯ä¸€è½®çš„æ¨¡å‹åŸå§‹å“åº”åˆ°æ—¥å¿—ï¼ˆå¤šè½®é—®é¢˜åœ¨è¿™é‡Œè®°å½•ï¼‰
            if raw is not None:
                try:
                    # ä½¿ç”¨å½“å‰é—®é¢˜IDå’Œè½®æ¬¡ï¼Œä¼ å…¥å®Œæ•´çš„æœ€ç»ˆæç¤ºè¯
                    log_model_response(
                        question_id=f"{question_id}_{round_key}",
                        question_num=question_num,
                        model_num=model_num,
                        model_name=api_config_name,
                        response=raw,
                        prompt=final_prompt  # ä½¿ç”¨å®Œæ•´çš„æœ€ç»ˆæç¤ºè¯ï¼Œè€Œä¸æ˜¯ question é¢„è§ˆ
                    )
                except Exception as e:
                    if self.debug_mode:
                        print(f"      âš ï¸ è®°å½•æ—¥å¿—å¤±è´¥ ({round_key}): {e}")

            # å°†æœ¬è½®é—®ç­”åŠ å…¥å†å²ï¼Œä¾›åç»­è½®æ¬¡å‚è€ƒ
            history_piece = f"{round_key} é—®é¢˜ï¼š{q_text}\n{round_key} ä½ çš„å›ç­”ï¼š{ans}"
            history_segments.append(history_piece)

        # ---- è¿”å›å­—å…¸æ ¼å¼ï¼Œä¿è¯æ¯ä¸ª round éƒ½æ˜¯ç‹¬ç«‹çš„ ----
        # å¤šè½®é¢˜è¿”å›å­—å…¸æ ¼å¼ï¼Œä¸ question å’Œ answer çš„æ ¼å¼ä¿æŒä¸€è‡´
        final_answer = answers if answers else {}
        final_process = processes if processes else {}

        return final_answer, final_process, total_time, last_raw_json, last_final_prompt
    
    def compare_three_models(self, qa_item: Dict) -> Dict:
        """
        å¯¹æ¯”ä¸‰ä¸ªæ¨¡å‹çš„ç­”æ¡ˆï¼ˆå¹¶è¡Œè°ƒç”¨ä»¥æé«˜é€Ÿåº¦ï¼‰
        
        Args:
            qa_item: åŒ…å« Q, Answer (GT), image_path ç­‰çš„å­—å…¸
                    å¦‚æœ qa_item ä¸­å·²åŒ…å« model1/model2/model3 å­—æ®µä¸”æœ‰æœ‰æ•ˆç­”æ¡ˆï¼Œåˆ™è·³è¿‡è¯¥æ¨¡å‹çš„è°ƒç”¨
        
        Returns:
            æ›´æ–°åçš„qa_itemï¼ŒåŒ…å«ä¸‰ä¸ªæ¨¡å‹çš„ç­”æ¡ˆå’Œå¯¹æ¯”ç»“æœ
        """
        question = qa_item.get("Q", "")
        question_rounds = qa_item.get("Q_rounds", None)
        is_multi_round = isinstance(question_rounds, dict)
        gt_answer = qa_item.get("Answer", "")  # GTç­”æ¡ˆ
        image_path = qa_item.get("image_path", "")
        question_id = qa_item.get("id", qa_item.get("question_id", "unknown"))  # è·å–é—®é¢˜IDç”¨äºæ—¥å¿—
        
        # ä¿å­˜å½“å‰é—®é¢˜IDï¼ˆç”¨äºå¤šè½®é—®é¢˜çš„æ—¥å¿—è®°å½•ï¼‰
        self.current_question_id = str(question_id)
        
        # å¢åŠ é—®é¢˜è®¡æ•°
        self.question_count += 1
        is_first_question = (self.question_count == 1)
        
        # è®°å½•é—®é¢˜å¼€å§‹ï¼ˆç”¨äºæ—¥å¿—é¡ºåºï¼‰
        log_question_start(question_id=str(question_id), question_num=self.question_count, 
                          is_multi_round=is_multi_round, question_preview=str(question_rounds if is_multi_round else question)[:200])
        
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç»“æœï¼ˆç”¨äºé”™è¯¯é‡è¯•ï¼‰
        existing_results = {}
        for model_num in [1, 2, 3]:
            model_key = f"model{model_num}"
            if model_key in qa_item and isinstance(qa_item[model_key], dict):
                existing_answer = qa_item[model_key].get("answer", "")
                # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆç­”æ¡ˆ
                if existing_answer and (isinstance(existing_answer, str) and existing_answer.strip()) or \
                   (isinstance(existing_answer, dict) and existing_answer):
                    existing_results[model_num] = qa_item[model_key]
                    print(f"    æ¨¡å‹{model_num} å·²æœ‰ç»“æœï¼Œè·³è¿‡è°ƒç”¨")
        
        # ========== å¹¶è¡Œè°ƒç”¨ä¸‰ä¸ªæ¨¡å‹ ==========
        need_call = False
        tasks = []
        if self.model1_enabled and 1 not in existing_results:
            tasks.append((1, self.model1_api_config_name, "æ¨¡å‹1"))
            need_call = True
        if self.model2_enabled and 2 not in existing_results:
            tasks.append((2, self.model2_api_config_name, "æ¨¡å‹2"))
            need_call = True
        if self.model3_enabled and 3 not in existing_results:
            tasks.append((3, self.model3_api_config_name, "æ¨¡å‹3"))
            need_call = True
        
        if need_call:
            print(f"    å¹¶è¡Œè°ƒç”¨ {len(tasks)} ä¸ªæ¨¡å‹...")
        
        start_time = time.time()
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œæ‰§è¡Œ
        results = {}
        
        # å…ˆæ·»åŠ å·²æœ‰ç»“æœ
        for model_num, existing_data in existing_results.items():
            results[model_num] = {
                "answer": existing_data.get("answer", ""),
                "process": existing_data.get("process", ""),
                "response_time": existing_data.get("response_time", 0.0),
                "raw_response_json": None,
                "final_prompt": ""
            }
        
        # è°ƒç”¨éœ€è¦å¤„ç†çš„æ¨¡å‹
        if tasks:
            with ThreadPoolExecutor(max_workers=3) as executor:
                # æäº¤æ‰€æœ‰ä»»åŠ¡
                future_to_model = {}
                for model_num, api_config_name, model_name in tasks:
                    print(f"    æäº¤{model_name}ä»»åŠ¡ (api_config: {api_config_name})...")
                    if is_multi_round:
                        # å¤šè½®é¢˜ï¼šæ ¼å¼è¦æ±‚å·²åœ¨ PROMPT_TEMPLATE ä¸­ç»Ÿä¸€è¯´æ˜ï¼Œä¸éœ€è¦é¢å¤–ä¼ é€’
                        future = executor.submit(
                            self.get_model_answer_multi_round,
                            model_num, api_config_name, True, question_rounds, image_path, 
                            str(question_id), self.question_count, ""
                        )
                    else:
                        future = executor.submit(
                            self.get_model_answer,
                            model_num, api_config_name, True, question, image_path
                        )
                    future_to_model[future] = (model_num, model_name)
                
                # æ”¶é›†ç»“æœ
                for future in as_completed(future_to_model):
                    model_num, model_name = future_to_model[future]
                    try:
                        result = future.result()
                        # å¤„ç†è¿”å›å€¼ï¼šå¯èƒ½æ˜¯ (answer, process, response_time, raw_json) æˆ– (answer, process, response_time, raw_json, final_prompt)
                        if len(result) >= 4:
                            answer = result[0]
                            process = result[1]
                            response_time = result[2]
                            raw_response_json = result[3]
                            final_prompt = result[4] if len(result) > 4 else str(question)  # å®Œæ•´çš„æœ€ç»ˆæç¤ºè¯
                        else:
                            # å…¼å®¹æ—§æ ¼å¼
                            answer, process, response_time, raw_response_json = result[:4]
                            final_prompt = str(question)
                        
                        results[model_num] = {
                            "answer": answer,
                            "process": process,
                            "response_time": response_time,
                            "raw_response_json": raw_response_json,
                            "final_prompt": final_prompt  # ä¿å­˜å®Œæ•´æç¤ºè¯
                        }
                        
                        # è®°å½•æ¨¡å‹åŸå§‹å“åº”åˆ°æ—¥å¿—ï¼ˆå•è½®é—®é¢˜åœ¨è¿™é‡Œè®°å½•ï¼‰
                        if raw_response_json is not None and not is_multi_round:
                            try:
                                api_config_name = self.model1_api_config_name if model_num == 1 else (self.model2_api_config_name if model_num == 2 else self.model3_api_config_name)
                                log_model_response(
                                    question_id=str(question_id),
                                    question_num=self.question_count,
                                    model_num=model_num,
                                    model_name=api_config_name,
                                    response=raw_response_json,
                                    prompt=final_prompt  # ä½¿ç”¨å®Œæ•´çš„æœ€ç»ˆæç¤ºè¯
                                )
                            except Exception as e:
                                if self.debug_mode:
                                    print(f"      âš ï¸ è®°å½•æ—¥å¿—å¤±è´¥ ({model_name}): {e}")
                        
                        print(f"    {model_name}å®Œæˆï¼Œè€—æ—¶: {response_time:.2f}ç§’")
                    except Exception as e:
                        print(f"    {model_name}è°ƒç”¨å¤±è´¥: {e}")
                        results[model_num] = {
                            "answer": "",
                            "process": "",
                            "response_time": 0.0,
                            "raw_response_json": None
                        }
        
        total_time = time.time() - start_time
        if need_call:
            print(f"    æ¨¡å‹è°ƒç”¨å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f}ç§’")
        
        # æå–ç»“æœ
        # æ³¨æ„ï¼šå¤šè½®é¢˜è¿”å›å­—å…¸æ ¼å¼ï¼Œå•è½®é¢˜è¿”å›å­—ç¬¦ä¸²æ ¼å¼
        answer1 = results.get(1, {}).get("answer", "" if not is_multi_round else {})
        process1 = results.get(1, {}).get("process", "" if not is_multi_round else {})
        time1 = results.get(1, {}).get("response_time", 0.0)
        raw_json1 = results.get(1, {}).get("raw_response_json", None)
        
        answer2 = results.get(2, {}).get("answer", "" if not is_multi_round else {})
        process2 = results.get(2, {}).get("process", "" if not is_multi_round else {})
        time2 = results.get(2, {}).get("response_time", 0.0)
        raw_json2 = results.get(2, {}).get("raw_response_json", None)
        
        answer3 = results.get(3, {}).get("answer", "" if not is_multi_round else {})
        process3 = results.get(3, {}).get("process", "" if not is_multi_round else {})
        time3 = results.get(3, {}).get("response_time", 0.0)
        raw_json3 = results.get(3, {}).get("raw_response_json", None)
        
        # æ³¨æ„ï¼šfinal_prompt å·²åœ¨æ—¥å¿—è®°å½•æ—¶ä½¿ç”¨ï¼Œè¿™é‡Œä¸éœ€è¦æå–
        
        # å¦‚æœæ˜¯ç¬¬ä¸€ä¸ªé—®é¢˜ï¼Œæ‰“å°ä¸‰ä¸ªæ¨¡å‹çš„åŸå§‹JSONå“åº”
        if is_first_question:
            print("\n" + "=" * 80)
            print("ğŸ“‹ ç¬¬ä¸€ä¸ªé—®é¢˜çš„åŸå§‹APIå“åº”JSON:")
            print("=" * 80)
            
            if raw_json1 is not None:
                print("\nã€æ¨¡å‹1 åŸå§‹å“åº”JSONã€‘")
                print(json.dumps(raw_json1, ensure_ascii=False, indent=2))
            else:
                print("\nã€æ¨¡å‹1 åŸå§‹å“åº”JSONã€‘: æ— ï¼ˆå¯èƒ½è°ƒç”¨å¤±è´¥æˆ–æµå¼è¾“å‡ºï¼‰")
            
            if raw_json2 is not None:
                print("\nã€æ¨¡å‹2 åŸå§‹å“åº”JSONã€‘")
                print(json.dumps(raw_json2, ensure_ascii=False, indent=2))
            else:
                print("\nã€æ¨¡å‹2 åŸå§‹å“åº”JSONã€‘: æ— ï¼ˆå¯èƒ½è°ƒç”¨å¤±è´¥æˆ–æµå¼è¾“å‡ºï¼‰")
            
            if raw_json3 is not None:
                print("\nã€æ¨¡å‹3 åŸå§‹å“åº”JSONã€‘")
                print(json.dumps(raw_json3, ensure_ascii=False, indent=2))
            else:
                print("\nã€æ¨¡å‹3 åŸå§‹å“åº”JSONã€‘: æ— ï¼ˆå¯èƒ½è°ƒç”¨å¤±è´¥æˆ–æµå¼è¾“å‡ºï¼‰")
            
            print("=" * 80 + "\n")
        
        # ä¿å­˜æ¨¡å‹ç­”æ¡ˆï¼ˆç»Ÿä¸€æ ¼å¼ï¼Œä½¿ç”¨è‡ªå®šä¹‰æ¨¡å‹åç§°ï¼‰
        # ç»Ÿä¸€ä½¿ç”¨ process å­—æ®µï¼Œä¸å†ä½¿ç”¨ cotï¼ˆä¿æŒå‘åå…¼å®¹ä½†æ ‡å‡†åŒ–ä¸º processï¼‰
        # å¤šè½®é¢˜ï¼šanswer å’Œ process ä¸ºå­—å…¸æ ¼å¼ï¼›å•è½®é¢˜ï¼šä¸ºå­—ç¬¦ä¸²æ ¼å¼
        # ç›´æ¥ä½¿ç”¨æå–çš„å€¼ï¼Œget æ–¹æ³•å·²ç»æä¾›äº†æ­£ç¡®çš„é»˜è®¤å€¼
        qa_item["model1"] = {
            "enabled": self.model1_enabled,
            "answer": answer1,
            "process": process1,
            "model_name": self.model1_api_config_name or "",
            "response_time": time1 if time1 > 0 else 0.0
        }
        
        qa_item["model2"] = {
            "enabled": self.model2_enabled,
            "answer": answer2,
            "process": process2,
            "model_name": self.model2_api_config_name or "",
            "response_time": time2 if time2 > 0 else 0.0
        }
        
        qa_item["model3"] = {
            "enabled": self.model3_enabled,
            "answer": answer3,
            "process": process3,
            "model_name": self.model3_api_config_name or "",
            "response_time": time3 if time3 > 0 else 0.0
        }
        
        # åªç»Ÿè®¡å¯ç”¨çš„æ¨¡å‹
        enabled_answers = []
        if self.model1_enabled:
            enabled_answers.append(answer1)
        if self.model2_enabled:
            enabled_answers.append(answer2)
        if self.model3_enabled:
            enabled_answers.append(answer3)
        
        # å¯¹æ¯”ç»“æœï¼šåªç»Ÿè®¡ä¸GTä¸€è‡´çš„æ¨¡å‹æ•°é‡
        agreement_count = count_agreement(enabled_answers, gt_answer) if enabled_answers and gt_answer else 0
        
        qa_item["comparison"] = {
            "agreement_with_gt": agreement_count
        }
        
        return qa_item

